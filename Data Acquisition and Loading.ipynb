{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics over Reddit Data\n",
    "\n",
    "For this project we are going to ingest Reddit posts, process the data and perform computational linguistics over the Reddit posts.\n",
    "\n",
    "### From the site:\n",
    "\n",
    "reddit: https://www.reddit.com/  \n",
    "Reddit gives you the best of the Internet in one place. Get a constantly updating feed of breaking news, fun stories, pics, memes, and videos just for you.\n",
    "\n",
    "\n",
    "### From Wikipedia:\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. \n",
    "Registered members submit content to the site such as links, text posts, and images, \n",
    "which are then voted up or down by other members. \n",
    "Posts are organized by subject into user-created boards called \"subreddits\", \n",
    "which cover a variety of topics including news, science, movies, video games, music, books, fitness, food, and image-sharing. \n",
    "Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough votes, ultimately on the site's front page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Reddit API\n",
    "\n",
    "For fetching Reddit data using API, we will be using a Python wrapper to Reddit API: [PRAW: The Python Reddit API Wrapper](https://github.com/praw-dev/praw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required headers\n",
    "import pandas as pd\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.4.0 of praw is outdated. Version 7.7.1 was released Tuesday July 11, 2023.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='****', \n",
    "                     client_secret='****', \n",
    "                     user_agent='***')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 hot posts from the MachineLearning subreddit\n",
    "hot_posts = reddit.subreddit('dataengineering').hot(limit=10)  # hot posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Submission(id='16x4y7c'),\n",
       " Submission(id='167b3ep'),\n",
       " Submission(id='1771qcz'),\n",
       " Submission(id='17754gk'),\n",
       " Submission(id='1771xvz'),\n",
       " Submission(id='17714nd'),\n",
       " Submission(id='1774azf'),\n",
       " Submission(id='177373h'),\n",
       " Submission(id='17735oa'),\n",
       " Submission(id='177apz9')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts = list(hot_posts)  \n",
    "all_posts\n",
    "# this line will initiate the fetching of posts as PRAW use a lazy approach (i.e, fetch when required)\n",
    "# this part is done to avoid calling Reddit API multiple times while developing our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 16x4y7c\n",
      "title : Monthly General Discussion - Oct 2023\n",
      "url : https://www.reddit.com/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/\n",
      "author : AutoModerator <class 'str'>\n",
      "score : 2 <class 'int'> \n",
      "subreddit : dataengineering <class 'praw.models.reddit.subreddit.Subreddit'> \n",
      "num_comments : 9\n",
      "body : This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n",
      "\n",
      "Examples:\n",
      "\n",
      "* What are you working on this month?\n",
      "* What was something you accomplished?\n",
      "* What was something you learned recently?\n",
      "* What is something frustrating you currently?\n",
      "\n",
      "As always, sub rules apply. Please be respectful and stay curious.\n",
      "\n",
      "**Community Links:**\n",
      "\n",
      "* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n",
      "* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n",
      "* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n",
      "* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)\n",
      "created : 1696176058.0\n",
      "link_flair_text : Discussion\n"
     ]
    }
   ],
   "source": [
    "for post in all_posts:\n",
    "    print(f\"id : {post.id}\")\n",
    "    print(f\"title : {post.title}\")\n",
    "    print(f\"url : {post.url}\")\n",
    "    print(f\"author : {str(post.author)} {type(str(post.author))}\")\n",
    "    print(f\"score : {post.score} {type(post.score)} \")\n",
    "    print(f\"subreddit : {post.subreddit} {type(post.subreddit)} \")\n",
    "    print(f\"num_comments : {post.num_comments}\")\n",
    "    print(f\"body : {post.selftext}\")\n",
    "    print(f\"created : {post.created}\")\n",
    "    print(f\"link_flair_text : {post.link_flair_text}\")\n",
    "    break  # break the loop after printing information about the first post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Reddits\n",
    "\n",
    "As described above, sub-reddits are communities organized around particular topics.\n",
    "\n",
    "Some example sub-reddits:\n",
    " * https://www.reddit.com/r/datascience/\n",
    " * https://www.reddit.com/r/MachineLearning/\n",
    " * https://www.reddit.com/r/LanguageTechnology/\n",
    " * https://www.reddit.com/r/NLP/\n",
    " * https://www.reddit.com/r/Python/\n"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Database Design:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the comprehensive analysis of Reddit data in this project, I have established a structured database using three SQL tables. The initial table, named \"posts_t\", has accommodated the first 100 entries obtained through the ETL process. Subsequently, I created a second table, \"posts_newt\", and repeated the process to store an additional set of 878 entries. To consolidate the data from both tables, I executed a union operation, resulting in a combined result stored in a table named \"combined_posts\".\n",
    "The schema for all three tables is identical and includes the following columns: (id, title, url, author, score, subreddit, num_comments, body, date_time, link_flair_text, preprocessed_body, sentiment_score).  \n",
    "For a visual representation of the structure, please refer to the ER diagram provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ERD (Entity-Relationship Diagram) for the project is as follows:\n",
    "\n",
    "![ERD-HERE](proj_erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement the database in your PostgreSQL schema\n",
    "\n",
    "You can choose any of the three ways to implement your database. \n",
    "\n",
    "* sql magic \n",
    "* sql terminal \n",
    "* psycopg2 or sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 2, I followed a series of steps to enhance the functionality of our database. Initially, I established a table called \"posts_t\". Subsequently, I implemented a trigger mechanism to monitor and record any modifications made to the \"body\" field of Reddit posts. Lastly, I introduced a GIST vector index for the \"preprocessed_body\" field, which I had preprocessed to improve search performance. GIST index is helpful to significantly improves query performance and search capabilities for these types of data. To ensure data integrity and consistency, I conducted a thorough review of the table schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Password and hit enter········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "# Initialize some variables\n",
    "mysso=\"***\"    # this is also your schema name. \n",
    "schema='***' \n",
    "hostname='***'\n",
    "database='***'\n",
    "\n",
    "mypasswd = getpass.getpass(\"Type Password and hit enter\")\n",
    "connection_string = f\"postgres://{mysso}:{mypasswd}@{hostname}/{database}\"\n",
    "\n",
    "%load_ext sql\n",
    "%sql $connection_string \n",
    "\n",
    "# Then remove the password from computer memory\n",
    "del mypasswd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS posts_t;\n",
    "\n",
    "CREATE TABLE posts_t(\n",
    "    id TEXT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    url TEXT,\n",
    "    author TEXT,\n",
    "    score INTEGER,\n",
    "    subreddit TEXT,\n",
    "    num_comments INTEGER,\n",
    "    body TEXT,\n",
    "    date_time TIMESTAMP,\n",
    "    link_flair_text TEXT,\n",
    "    preprocessed_body TEXT,\n",
    "    sentiment_score double precision\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TRIGGER IF EXISTS tsv_gist_update on posts_t;\n",
    "    \n",
    "CREATE TRIGGER tsv_gist_update \n",
    "    BEFORE INSERT OR UPDATE\n",
    "    ON posts_t \n",
    "    FOR EACH ROW \n",
    "    EXECUTE PROCEDURE\n",
    "    tsvector_update_trigger(body_tsv_gist,'pg_catalog.english', body);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "-- TS_Vector for GIST INDEX\n",
    "ALTER TABLE posts_t\n",
    "  ADD COLUMN body_tsv_gist tsvector;\n",
    "\n",
    "-- now update the above column by parsing the content column. Note: the following is only required if we\n",
    "-- already have some rows in the table.\n",
    "\n",
    "UPDATE posts_t \n",
    "SET body_tsv_gist = to_tsvector('pg_catalog.english', preprocessed_body);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>title</th>\n",
       "        <th>url</th>\n",
       "        <th>author</th>\n",
       "        <th>score</th>\n",
       "        <th>subreddit</th>\n",
       "        <th>num_comments</th>\n",
       "        <th>body</th>\n",
       "        <th>date_time</th>\n",
       "        <th>link_flair_text</th>\n",
       "        <th>preprocessed_body</th>\n",
       "        <th>sentiment_score</th>\n",
       "        <th>body_tsv_gist</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM mrhmr.posts_t;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement cells of Python Code that\n",
    "\n",
    "* collect the latest posts from a subreddit of your choice (should be text-dominant not image/video) and collect at least 500 posts (if possible),\n",
    "* processes the messages to extract id, title, link, author, subreddit, tag/flair, timestamp, etc. \n",
    "* process the text for IR, and\n",
    "* perform computational linguistics (e.g., get sentiment scores)\n",
    "* then insert the data into your database.\n",
    "\n",
    "\n",
    "Notes: \n",
    "* Each call to Reddit API returns 100 entries max. If we set a limit of more than 100, PRAW will handle multiple API calls internally and lazily fetches data. Check obfuscation and API limitation in https://praw.readthedocs.io/en/v3.6.2/pages/getting_started.html. \n",
    "* Develop and test your code with less than 100 messages from a subreddit. Then increase the limit and add few more subreddits. \n",
    "* While loading the table, test with one row \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, my process began with scraping data from 100 different subreddits and transforming it into a DataFrame. During my analysis, I noticed that certain columns, such as 'body' and 'title', required some data preprocessing. I applied the techniques I had previously learned from other modules to clean and refine these columns.\n",
    "\n",
    "Additionally, I identified issues with the 'date_time' column, which was not in the desired format. While working on the 'author' and 'subreddit' columns, I encountered discrepancies in the data. To address these concerns, I first formatted the 'date_time' column to a timestamp format. I then took steps to ensure that the 'author' and 'subreddit' columns in the DataFrame contained easily interpretable values (names or display names) or 'None' in cases where the relevant attributes were missing. \n",
    "Subsequently, I computed a compound score by employing a sentiment analyzer on the 'preprocessed_body' column. Here, I choose to preprocess the body text then apply sentiment analyser on the preprocessed text because it tends to yield more accurate and consistent results. Preprocessing helps remove noise and standardizes the text, making it easier for sentiment analysis models to understand and classify sentiment. This score was then stored in the DataFrame under the column name 'sentiment_score'. Having completed these data preparation tasks, I successfully loaded the refined 100 entries into an SQL table named 'posts_t'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code in this cell\n",
    "## ------------------------\n",
    "reddit = praw.Reddit(client_id='***', \n",
    "                     client_secret='***', \n",
    "                     user_agent='***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                              title  \\\n",
      "0   16x4y7c              Monthly General Discussion - Oct 2023   \n",
      "1   167b3ep             Quarterly Salary Discussion - Sep 2023   \n",
      "2   1771qcz                          Introducing Dagster Pipes   \n",
      "3   17754gk  What python skills I should focus on for a Sen...   \n",
      "4   1771xvz  Why should I deploy data observability for our...   \n",
      "..      ...                                                ...   \n",
      "95  174xlkf                                       Data lineage   \n",
      "96  174qp1c                      On-prem setup for a lakehouse   \n",
      "97  174aeb1                   Data engineer from ETL developer   \n",
      "98  1750hbf                                        Data scrape   \n",
      "99  174mh1z  How do I move 30GB from sharepoint to Azure st...   \n",
      "\n",
      "                                                  url                author  \\\n",
      "0   https://www.reddit.com/r/dataengineering/comme...         AutoModerator   \n",
      "1   https://www.reddit.com/r/dataengineering/comme...         AutoModerator   \n",
      "2               https://dagster.io/blog/dagster-pipes              schrockn   \n",
      "3   https://www.reddit.com/r/dataengineering/comme...               mcfryme   \n",
      "4   https://www.reddit.com/r/dataengineering/comme...                de4all   \n",
      "..                                                ...                   ...   \n",
      "95  https://www.reddit.com/r/dataengineering/comme...    Last-Marzipan-2808   \n",
      "96  https://www.reddit.com/r/dataengineering/comme...                s0uha1   \n",
      "97  https://www.reddit.com/r/dataengineering/comme...  Charming_Function_35   \n",
      "98  https://www.reddit.com/r/dataengineering/comme...       virtualtechcart   \n",
      "99  https://www.reddit.com/r/dataengineering/comme...          drollerfoot7   \n",
      "\n",
      "    score        subreddit  num_comments  \\\n",
      "0       2  dataengineering             9   \n",
      "1      84  dataengineering           222   \n",
      "2      27  dataengineering             1   \n",
      "3      17  dataengineering            18   \n",
      "4       6  dataengineering            16   \n",
      "..    ...              ...           ...   \n",
      "95      0  dataengineering             0   \n",
      "96      2  dataengineering            10   \n",
      "97     29  dataengineering            32   \n",
      "98      0  dataengineering             1   \n",
      "99      3  dataengineering             6   \n",
      "\n",
      "                                                 body     date_time  \\\n",
      "0   This thread is a place where you can share thi...  1.696176e+09   \n",
      "1   https://preview.redd.it/ia7kdykk8dlb1.png?widt...  1.693584e+09   \n",
      "2                                                      1.697212e+09   \n",
      "3   I have 5+ years of Data Analysis experience. I...  1.697221e+09   \n",
      "4   I can write manual scripts and run DAGs, why s...  1.697212e+09   \n",
      "..                                                ...           ...   \n",
      "95  Is it possible to somehow get data lineage fro...  1.696976e+09   \n",
      "96  I'm working in a medium-sized company and due ...  1.696958e+09   \n",
      "97  I have been etl developer for 15 years(SSIS) a...  1.696905e+09   \n",
      "98  Anyone interested in talking about data scraping?  1.696983e+09   \n",
      "99  Hi, my company has about 30GB total on a share...  1.696948e+09   \n",
      "\n",
      "   link_flair_text  \n",
      "0       Discussion  \n",
      "1           Career  \n",
      "2      Open Source  \n",
      "3        Interview  \n",
      "4       Discussion  \n",
      "..             ...  \n",
      "95            Help  \n",
      "96      Discussion  \n",
      "97      Discussion  \n",
      "98            Help  \n",
      "99            Help  \n",
      "\n",
      "[100 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#extracting 100 posts from the subreddit\n",
    "posts = []\n",
    "dm_subreddit = reddit.subreddit('dataengineering')\n",
    "for post in dm_subreddit.hot(limit=100):\n",
    "    posts.append([ post.id, post.title, post.url, post.author, post.score, post.subreddit, post.num_comments, post.selftext, post.created, post.link_flair_text])\n",
    "posts_df = pd.DataFrame(posts,columns=['id', 'title', 'url', 'author', 'score', 'subreddit', 'num_comments', 'body', 'date_time', 'link_flair_text'])\n",
    "print(posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16x4y7c</td>\n",
       "      <td>Monthly General Discussion - Oct 2023</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>2</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>9</td>\n",
       "      <td>This thread is a place where you can share thi...</td>\n",
       "      <td>2023-10-01 16:00:58</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167b3ep</td>\n",
       "      <td>Quarterly Salary Discussion - Sep 2023</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>84</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>222</td>\n",
       "      <td>https://preview.redd.it/ia7kdykk8dlb1.png?widt...</td>\n",
       "      <td>2023-09-01 16:01:00</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1771qcz</td>\n",
       "      <td>Introducing Dagster Pipes</td>\n",
       "      <td>https://dagster.io/blog/dagster-pipes</td>\n",
       "      <td>schrockn</td>\n",
       "      <td>27</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-13 15:45:18</td>\n",
       "      <td>Open Source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17754gk</td>\n",
       "      <td>What python skills I should focus on for a Sen...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>mcfryme</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>18</td>\n",
       "      <td>I have 5+ years of Data Analysis experience. I...</td>\n",
       "      <td>2023-10-13 18:20:38</td>\n",
       "      <td>Interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1771xvz</td>\n",
       "      <td>Why should I deploy data observability for our...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>de4all</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>I can write manual scripts and run DAGs, why s...</td>\n",
       "      <td>2023-10-13 15:54:58</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  16x4y7c              Monthly General Discussion - Oct 2023   \n",
       "1  167b3ep             Quarterly Salary Discussion - Sep 2023   \n",
       "2  1771qcz                          Introducing Dagster Pipes   \n",
       "3  17754gk  What python skills I should focus on for a Sen...   \n",
       "4  1771xvz  Why should I deploy data observability for our...   \n",
       "\n",
       "                                                 url         author  score  \\\n",
       "0  https://www.reddit.com/r/dataengineering/comme...  AutoModerator      2   \n",
       "1  https://www.reddit.com/r/dataengineering/comme...  AutoModerator     84   \n",
       "2              https://dagster.io/blog/dagster-pipes       schrockn     27   \n",
       "3  https://www.reddit.com/r/dataengineering/comme...        mcfryme     17   \n",
       "4  https://www.reddit.com/r/dataengineering/comme...         de4all      6   \n",
       "\n",
       "         subreddit  num_comments  \\\n",
       "0  dataengineering             9   \n",
       "1  dataengineering           222   \n",
       "2  dataengineering             1   \n",
       "3  dataengineering            18   \n",
       "4  dataengineering            16   \n",
       "\n",
       "                                                body            date_time  \\\n",
       "0  This thread is a place where you can share thi...  2023-10-01 16:00:58   \n",
       "1  https://preview.redd.it/ia7kdykk8dlb1.png?widt...  2023-09-01 16:01:00   \n",
       "2                                                     2023-10-13 15:45:18   \n",
       "3  I have 5+ years of Data Analysis experience. I...  2023-10-13 18:20:38   \n",
       "4  I can write manual scripts and run DAGs, why s...  2023-10-13 15:54:58   \n",
       "\n",
       "  link_flair_text  \n",
       "0      Discussion  \n",
       "1          Career  \n",
       "2     Open Source  \n",
       "3       Interview  \n",
       "4      Discussion  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The time attribute is not properly formatted, this code converts it to date-time format\n",
    "def unix_timestamp_to_datetime(unix_timestamp):\n",
    "    return datetime.utcfromtimestamp(unix_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "posts_df['date_time'] = posts_df['date_time'].apply(unix_timestamp_to_datetime)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16x4y7c</td>\n",
       "      <td>Monthly General Discussion - Oct 2023</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>2</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>9</td>\n",
       "      <td>This thread is a place where you can share thi...</td>\n",
       "      <td>2023-10-01 16:00:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>thread place share things might warrant thread...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167b3ep</td>\n",
       "      <td>Quarterly Salary Discussion - Sep 2023</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>84</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>222</td>\n",
       "      <td>https://preview.redd.it/ia7kdykk8dlb1.png?widt...</td>\n",
       "      <td>2023-09-01 16:01:00</td>\n",
       "      <td>Career</td>\n",
       "      <td>recurring thread happens quarterly created hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1771qcz</td>\n",
       "      <td>Introducing Dagster Pipes</td>\n",
       "      <td>https://dagster.io/blog/dagster-pipes</td>\n",
       "      <td>schrockn</td>\n",
       "      <td>27</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-13 15:45:18</td>\n",
       "      <td>Open Source</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17754gk</td>\n",
       "      <td>What python skills I should focus on for a Sen...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>mcfryme</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>18</td>\n",
       "      <td>I have 5+ years of Data Analysis experience. I...</td>\n",
       "      <td>2023-10-13 18:20:38</td>\n",
       "      <td>Interview</td>\n",
       "      <td>years data analysis experience pretty good sql...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1771xvz</td>\n",
       "      <td>Why should I deploy data observability for our...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>de4all</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>I can write manual scripts and run DAGs, why s...</td>\n",
       "      <td>2023-10-13 15:54:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>write manual scripts run dags spends expensive...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  16x4y7c              Monthly General Discussion - Oct 2023   \n",
       "1  167b3ep             Quarterly Salary Discussion - Sep 2023   \n",
       "2  1771qcz                          Introducing Dagster Pipes   \n",
       "3  17754gk  What python skills I should focus on for a Sen...   \n",
       "4  1771xvz  Why should I deploy data observability for our...   \n",
       "\n",
       "                                                 url         author  score  \\\n",
       "0  https://www.reddit.com/r/dataengineering/comme...  AutoModerator      2   \n",
       "1  https://www.reddit.com/r/dataengineering/comme...  AutoModerator     84   \n",
       "2              https://dagster.io/blog/dagster-pipes       schrockn     27   \n",
       "3  https://www.reddit.com/r/dataengineering/comme...        mcfryme     17   \n",
       "4  https://www.reddit.com/r/dataengineering/comme...         de4all      6   \n",
       "\n",
       "         subreddit  num_comments  \\\n",
       "0  dataengineering             9   \n",
       "1  dataengineering           222   \n",
       "2  dataengineering             1   \n",
       "3  dataengineering            18   \n",
       "4  dataengineering            16   \n",
       "\n",
       "                                                body            date_time  \\\n",
       "0  This thread is a place where you can share thi...  2023-10-01 16:00:58   \n",
       "1  https://preview.redd.it/ia7kdykk8dlb1.png?widt...  2023-09-01 16:01:00   \n",
       "2                                                     2023-10-13 15:45:18   \n",
       "3  I have 5+ years of Data Analysis experience. I...  2023-10-13 18:20:38   \n",
       "4  I can write manual scripts and run DAGs, why s...  2023-10-13 15:54:58   \n",
       "\n",
       "  link_flair_text                                  preprocessed_body  \n",
       "0      Discussion  thread place share things might warrant thread...  \n",
       "1          Career  recurring thread happens quarterly created hel...  \n",
       "2     Open Source                                                     \n",
       "3       Interview  years data analysis experience pretty good sql...  \n",
       "4      Discussion  write manual scripts run dags spends expensive...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As 'body' of the subreddit contains web link, numbers and other features, in this block 'body' of the subreddits has been\n",
    "#preprocessed and saved it to another column named 'preprocesssed_body'\n",
    "\n",
    "posts_df['preprocessed_body'] = posts_df['body']\n",
    "contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"I'm\": \"i am\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i've\": \"i have\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # removing punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "     # removing numbers along with contractions\n",
    "    text = re.sub(r'\\d+|' + '|'.join(contractions.keys()), '', text)\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    text = \" \".join([word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    # removing single-character words\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "    \n",
    "    # removing newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "posts_df['preprocessed_body'] = posts_df['preprocessed_body'].apply(preprocess_text)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16x4y7c</td>\n",
       "      <td>monthly general discussion oct</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>2</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>9</td>\n",
       "      <td>This thread is a place where you can share thi...</td>\n",
       "      <td>2023-10-01 16:00:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>thread place share things might warrant thread...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167b3ep</td>\n",
       "      <td>quarterly salary discussion sep</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>84</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>222</td>\n",
       "      <td>https://preview.redd.it/ia7kdykk8dlb1.png?widt...</td>\n",
       "      <td>2023-09-01 16:01:00</td>\n",
       "      <td>Career</td>\n",
       "      <td>recurring thread happens quarterly created hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1771qcz</td>\n",
       "      <td>introducing dagster pipes</td>\n",
       "      <td>https://dagster.io/blog/dagster-pipes</td>\n",
       "      <td>schrockn</td>\n",
       "      <td>27</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-13 15:45:18</td>\n",
       "      <td>Open Source</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17754gk</td>\n",
       "      <td>python skills focus senior data engineer techn...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>mcfryme</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>18</td>\n",
       "      <td>I have 5+ years of Data Analysis experience. I...</td>\n",
       "      <td>2023-10-13 18:20:38</td>\n",
       "      <td>Interview</td>\n",
       "      <td>years data analysis experience pretty good sql...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1771xvz</td>\n",
       "      <td>deploy data observability data stack</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>de4all</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>I can write manual scripts and run DAGs, why s...</td>\n",
       "      <td>2023-10-13 15:54:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>write manual scripts run dags spends expensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>174xlkf</td>\n",
       "      <td>data lineage</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Last-Marzipan-2808</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it possible to somehow get data lineage fro...</td>\n",
       "      <td>2023-10-10 22:06:05</td>\n",
       "      <td>Help</td>\n",
       "      <td>possible somehow get data lineage ssis package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>174qp1c</td>\n",
       "      <td>onprem setup lakehouse</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>s0uha1</td>\n",
       "      <td>2</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>10</td>\n",
       "      <td>I'm working in a medium-sized company and due ...</td>\n",
       "      <td>2023-10-10 17:21:14</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>im working mediumsized company due regulatory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>174aeb1</td>\n",
       "      <td>data engineer etl developer</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Charming_Function_35</td>\n",
       "      <td>29</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>32</td>\n",
       "      <td>I have been etl developer for 15 years(SSIS) a...</td>\n",
       "      <td>2023-10-10 02:36:13</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>etl developer yearsssis time update skills thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1750hbf</td>\n",
       "      <td>data scrape</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>virtualtechcart</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td>Anyone interested in talking about data scraping?</td>\n",
       "      <td>2023-10-11 00:13:57</td>\n",
       "      <td>Help</td>\n",
       "      <td>anyone interested talking data scraping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>174mh1z</td>\n",
       "      <td>move gb sharepoint azure storage</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>drollerfoot7</td>\n",
       "      <td>3</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>6</td>\n",
       "      <td>Hi, my company has about 30GB total on a share...</td>\n",
       "      <td>2023-10-10 14:21:36</td>\n",
       "      <td>Help</td>\n",
       "      <td>hi company gb total sharepoint site need get f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0   16x4y7c                     monthly general discussion oct   \n",
       "1   167b3ep                    quarterly salary discussion sep   \n",
       "2   1771qcz                          introducing dagster pipes   \n",
       "3   17754gk  python skills focus senior data engineer techn...   \n",
       "4   1771xvz               deploy data observability data stack   \n",
       "..      ...                                                ...   \n",
       "95  174xlkf                                       data lineage   \n",
       "96  174qp1c                             onprem setup lakehouse   \n",
       "97  174aeb1                        data engineer etl developer   \n",
       "98  1750hbf                                        data scrape   \n",
       "99  174mh1z                   move gb sharepoint azure storage   \n",
       "\n",
       "                                                  url                author  \\\n",
       "0   https://www.reddit.com/r/dataengineering/comme...         AutoModerator   \n",
       "1   https://www.reddit.com/r/dataengineering/comme...         AutoModerator   \n",
       "2               https://dagster.io/blog/dagster-pipes              schrockn   \n",
       "3   https://www.reddit.com/r/dataengineering/comme...               mcfryme   \n",
       "4   https://www.reddit.com/r/dataengineering/comme...                de4all   \n",
       "..                                                ...                   ...   \n",
       "95  https://www.reddit.com/r/dataengineering/comme...    Last-Marzipan-2808   \n",
       "96  https://www.reddit.com/r/dataengineering/comme...                s0uha1   \n",
       "97  https://www.reddit.com/r/dataengineering/comme...  Charming_Function_35   \n",
       "98  https://www.reddit.com/r/dataengineering/comme...       virtualtechcart   \n",
       "99  https://www.reddit.com/r/dataengineering/comme...          drollerfoot7   \n",
       "\n",
       "    score        subreddit  num_comments  \\\n",
       "0       2  dataengineering             9   \n",
       "1      84  dataengineering           222   \n",
       "2      27  dataengineering             1   \n",
       "3      17  dataengineering            18   \n",
       "4       6  dataengineering            16   \n",
       "..    ...              ...           ...   \n",
       "95      0  dataengineering             0   \n",
       "96      2  dataengineering            10   \n",
       "97     29  dataengineering            32   \n",
       "98      0  dataengineering             1   \n",
       "99      3  dataengineering             6   \n",
       "\n",
       "                                                 body            date_time  \\\n",
       "0   This thread is a place where you can share thi...  2023-10-01 16:00:58   \n",
       "1   https://preview.redd.it/ia7kdykk8dlb1.png?widt...  2023-09-01 16:01:00   \n",
       "2                                                      2023-10-13 15:45:18   \n",
       "3   I have 5+ years of Data Analysis experience. I...  2023-10-13 18:20:38   \n",
       "4   I can write manual scripts and run DAGs, why s...  2023-10-13 15:54:58   \n",
       "..                                                ...                  ...   \n",
       "95  Is it possible to somehow get data lineage fro...  2023-10-10 22:06:05   \n",
       "96  I'm working in a medium-sized company and due ...  2023-10-10 17:21:14   \n",
       "97  I have been etl developer for 15 years(SSIS) a...  2023-10-10 02:36:13   \n",
       "98  Anyone interested in talking about data scraping?  2023-10-11 00:13:57   \n",
       "99  Hi, my company has about 30GB total on a share...  2023-10-10 14:21:36   \n",
       "\n",
       "   link_flair_text                                  preprocessed_body  \n",
       "0       Discussion  thread place share things might warrant thread...  \n",
       "1           Career  recurring thread happens quarterly created hel...  \n",
       "2      Open Source                                                     \n",
       "3        Interview  years data analysis experience pretty good sql...  \n",
       "4       Discussion  write manual scripts run dags spends expensive...  \n",
       "..             ...                                                ...  \n",
       "95            Help  possible somehow get data lineage ssis package...  \n",
       "96      Discussion  im working mediumsized company due regulatory ...  \n",
       "97      Discussion  etl developer yearsssis time update skills thi...  \n",
       "98            Help            anyone interested talking data scraping  \n",
       "99            Help  hi company gb total sharepoint site need get f...  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing 'title' column\n",
    "def preprocess_text(text):\n",
    "    # removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # removing punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "     # removing numbers along with contractions\n",
    "    text = re.sub(r'\\d+|' + '|'.join(contractions.keys()), '', text)\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    text = \" \".join([word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    # removing single-character words\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "    \n",
    "    # removing newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "posts_df['title'] = posts_df['title'].apply(preprocess_text)\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16x4y7c</td>\n",
       "      <td>monthly general discussion oct</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>2</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>9</td>\n",
       "      <td>This thread is a place where you can share thi...</td>\n",
       "      <td>2023-10-01 16:00:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>thread place share things might warrant thread...</td>\n",
       "      <td>0.8316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167b3ep</td>\n",
       "      <td>quarterly salary discussion sep</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>84</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>222</td>\n",
       "      <td>https://preview.redd.it/ia7kdykk8dlb1.png?widt...</td>\n",
       "      <td>2023-09-01 16:01:00</td>\n",
       "      <td>Career</td>\n",
       "      <td>recurring thread happens quarterly created hel...</td>\n",
       "      <td>0.8957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1771qcz</td>\n",
       "      <td>introducing dagster pipes</td>\n",
       "      <td>https://dagster.io/blog/dagster-pipes</td>\n",
       "      <td>schrockn</td>\n",
       "      <td>27</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-13 15:45:18</td>\n",
       "      <td>Open Source</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17754gk</td>\n",
       "      <td>python skills focus senior data engineer techn...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>mcfryme</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>18</td>\n",
       "      <td>I have 5+ years of Data Analysis experience. I...</td>\n",
       "      <td>2023-10-13 18:20:38</td>\n",
       "      <td>Interview</td>\n",
       "      <td>years data analysis experience pretty good sql...</td>\n",
       "      <td>0.8481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1771xvz</td>\n",
       "      <td>deploy data observability data stack</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>de4all</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>I can write manual scripts and run DAGs, why s...</td>\n",
       "      <td>2023-10-13 15:54:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>write manual scripts run dags spends expensive...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  16x4y7c                     monthly general discussion oct   \n",
       "1  167b3ep                    quarterly salary discussion sep   \n",
       "2  1771qcz                          introducing dagster pipes   \n",
       "3  17754gk  python skills focus senior data engineer techn...   \n",
       "4  1771xvz               deploy data observability data stack   \n",
       "\n",
       "                                                 url         author  score  \\\n",
       "0  https://www.reddit.com/r/dataengineering/comme...  AutoModerator      2   \n",
       "1  https://www.reddit.com/r/dataengineering/comme...  AutoModerator     84   \n",
       "2              https://dagster.io/blog/dagster-pipes       schrockn     27   \n",
       "3  https://www.reddit.com/r/dataengineering/comme...        mcfryme     17   \n",
       "4  https://www.reddit.com/r/dataengineering/comme...         de4all      6   \n",
       "\n",
       "         subreddit  num_comments  \\\n",
       "0  dataengineering             9   \n",
       "1  dataengineering           222   \n",
       "2  dataengineering             1   \n",
       "3  dataengineering            18   \n",
       "4  dataengineering            16   \n",
       "\n",
       "                                                body            date_time  \\\n",
       "0  This thread is a place where you can share thi...  2023-10-01 16:00:58   \n",
       "1  https://preview.redd.it/ia7kdykk8dlb1.png?widt...  2023-09-01 16:01:00   \n",
       "2                                                     2023-10-13 15:45:18   \n",
       "3  I have 5+ years of Data Analysis experience. I...  2023-10-13 18:20:38   \n",
       "4  I can write manual scripts and run DAGs, why s...  2023-10-13 15:54:58   \n",
       "\n",
       "  link_flair_text                                  preprocessed_body  \\\n",
       "0      Discussion  thread place share things might warrant thread...   \n",
       "1          Career  recurring thread happens quarterly created hel...   \n",
       "2     Open Source                                                      \n",
       "3       Interview  years data analysis experience pretty good sql...   \n",
       "4      Discussion  write manual scripts run dags spends expensive...   \n",
       "\n",
       "   sentiment_score  \n",
       "0           0.8316  \n",
       "1           0.8957  \n",
       "2           0.0000  \n",
       "3           0.8481  \n",
       "4           0.0000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment analysis on 'preprocessed_body'  column, calculated compound score and saved it as 'sentiment_score'\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    sentiment_scores = sa.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "posts_df['sentiment_score'] = posts_df['preprocessed_body'].apply(calculate_sentiment)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted the 'author' and 'subreddit' column to standardize the data to readable format\n",
    "posts_df['author'] = posts_df['author'].apply(lambda redditor: redditor.name if hasattr(redditor, 'name') else None)\n",
    "posts_df['subreddit'] = posts_df['subreddit'].apply(lambda subreddit: subreddit.display_name if hasattr(subreddit, 'display_name') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    object\n",
       "title                 object\n",
       "url                   object\n",
       "author                object\n",
       "score                  int64\n",
       "subreddit             object\n",
       "num_comments           int64\n",
       "body                  object\n",
       "date_time             object\n",
       "link_flair_text       object\n",
       "preprocessed_body     object\n",
       "sentiment_score      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "#connection establishment\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import getpass\n",
    "mypasswd = getpass.getpass()\n",
    "username = '***'\n",
    "host = '***'\n",
    "database='***'\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# SQLAlchemy Connection Parameters\n",
    "postgres_db = {'drivername': '***',\n",
    "               'username': username,\n",
    "               'password': mypasswd,\n",
    "               'host': host,\n",
    "               'database' :database}\n",
    "engine = create_engine(URL(**postgres_db), echo=False)\n",
    "del mypasswd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.to_sql('posts_t', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>title</th>\n",
       "        <th>url</th>\n",
       "        <th>author</th>\n",
       "        <th>score</th>\n",
       "        <th>subreddit</th>\n",
       "        <th>num_comments</th>\n",
       "        <th>body</th>\n",
       "        <th>date_time</th>\n",
       "        <th>link_flair_text</th>\n",
       "        <th>preprocessed_body</th>\n",
       "        <th>sentiment_score</th>\n",
       "        <th>body_tsv_gist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>16x4y7c</td>\n",
       "        <td>monthly general discussion oct</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/</td>\n",
       "        <td>AutoModerator</td>\n",
       "        <td>2</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>9</td>\n",
       "        <td>This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.<br><br>Examples:<br><br>* What are you working on this month?<br>* What was something you accomplished?<br>* What was something you learned recently?<br>* What is something frustrating you currently?<br><br>As always, sub rules apply. Please be respectful and stay curious.<br><br>**Community Links:**<br><br>* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)<br>* [Data Engineering Events](https://dataengineering.wiki/Community/Events)<br>* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)<br>* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)</td>\n",
       "        <td>2023-10-01 16:00:58</td>\n",
       "        <td>Discussion</td>\n",
       "        <td>thread place share things might warrant thread automatically posted month find previous threads collection examples working month something accomplished something learned recently something frustrating currently always sub rules apply please respectful stay curious community links monthly newsletter data engineering events data engineering meetups get involved community</td>\n",
       "        <td>0.8316</td>\n",
       "        <td>&#x27;/)&#x27;:75 &#x27;/community/events)&#x27;:81 &#x27;/community/get+involved)&#x27;:95 &#x27;/community/meetups)&#x27;:87 &#x27;accomplish&#x27;:45 &#x27;alway&#x27;:59 &#x27;appli&#x27;:62 &#x27;automat&#x27;:20 &#x27;collect&#x27;:32 &#x27;communiti&#x27;:69,92 &#x27;curious&#x27;:68 &#x27;current&#x27;:57 &#x27;data&#x27;:76,82 &#x27;dataengineering.wiki&#x27;:80,86,94 &#x27;dataengineering.wiki/community/events)&#x27;:79 &#x27;dataengineering.wiki/community/get+involved)&#x27;:93 &#x27;dataengineering.wiki/community/meetups)&#x27;:85 &#x27;dataengineeringcommunity.substack.com&#x27;:74 &#x27;dataengineeringcommunity.substack.com/)&#x27;:73 &#x27;engin&#x27;:77,83 &#x27;event&#x27;:78 &#x27;exampl&#x27;:33 &#x27;find&#x27;:27 &#x27;frustrat&#x27;:55 &#x27;get&#x27;:88 &#x27;involv&#x27;:89 &#x27;learn&#x27;:50 &#x27;link&#x27;:70 &#x27;meetup&#x27;:84 &#x27;might&#x27;:12 &#x27;month&#x27;:23,40,71 &#x27;newslett&#x27;:72 &#x27;place&#x27;:5 &#x27;pleas&#x27;:63 &#x27;post&#x27;:21 &#x27;previous&#x27;:28 &#x27;recent&#x27;:51 &#x27;respect&#x27;:65 &#x27;rule&#x27;:61 &#x27;share&#x27;:9 &#x27;someth&#x27;:43,48,54 &#x27;stay&#x27;:67 &#x27;sub&#x27;:60 &#x27;thing&#x27;:10 &#x27;thread&#x27;:2,17,29 &#x27;warrant&#x27;:14 &#x27;work&#x27;:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>167b3ep</td>\n",
       "        <td>quarterly salary discussion sep</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/</td>\n",
       "        <td>AutoModerator</td>\n",
       "        <td>84</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>222</td>\n",
       "        <td>https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd<br><br>This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.<br><br># [Submit your salary here](https://tally.so/r/nraYkN)<br><br>&amp;#x200B;<br><br>If you&#x27;d like to share publicly as well you can optionally comment below and include the following:<br><br>1. Current title<br>2. Years of experience (YOE)<br>3. Location<br>4. Base salary &amp; currency (dollars, euro, pesos, etc.)<br>5. Bonuses/Equity (optional)<br>6. Industry (optional)<br>7. Tech stack (optional)</td>\n",
       "        <td>2023-09-01 16:01:00</td>\n",
       "        <td>Career</td>\n",
       "        <td>recurring thread happens quarterly created help increase transparency around salary compensation data engineering submit salary xb youd like share publicly well optionally comment include following current title years experience yoe location base salary currency dollars euro pesos etc bonusesequity optional industry optional tech stack optional</td>\n",
       "        <td>0.8957</td>\n",
       "        <td>&#x27;/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd&#x27;:3 &#x27;/r/nraykn)&#x27;:32 &#x27;1&#x27;:51 &#x27;2&#x27;:54 &#x27;3&#x27;:59 &#x27;4&#x27;:61 &#x27;5&#x27;:69 &#x27;6&#x27;:72 &#x27;7&#x27;:75 &#x27;around&#x27;:19 &#x27;base&#x27;:62 &#x27;bonuses/equity&#x27;:70 &#x27;comment&#x27;:45 &#x27;compens&#x27;:22 &#x27;creat&#x27;:14 &#x27;currenc&#x27;:64 &#x27;current&#x27;:52 &#x27;d&#x27;:35 &#x27;data&#x27;:24 &#x27;dollar&#x27;:65 &#x27;engin&#x27;:25 &#x27;etc&#x27;:68 &#x27;euro&#x27;:66 &#x27;experi&#x27;:57 &#x27;follow&#x27;:50 &#x27;happen&#x27;:10 &#x27;help&#x27;:16 &#x27;includ&#x27;:48 &#x27;increas&#x27;:17 &#x27;industri&#x27;:73 &#x27;like&#x27;:36 &#x27;locat&#x27;:60 &#x27;option&#x27;:44,71,74,78 &#x27;peso&#x27;:67 &#x27;preview.redd.it&#x27;:2 &#x27;preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd&#x27;:1 &#x27;public&#x27;:39 &#x27;quarter&#x27;:11 &#x27;recur&#x27;:7 &#x27;salari&#x27;:20,28,63 &#x27;share&#x27;:38 &#x27;stack&#x27;:77 &#x27;submit&#x27;:26 &#x27;tally.so&#x27;:31 &#x27;tally.so/r/nraykn)&#x27;:30 &#x27;tech&#x27;:76 &#x27;thread&#x27;:8 &#x27;titl&#x27;:53 &#x27;transpar&#x27;:18 &#x27;well&#x27;:41 &#x27;year&#x27;:55 &#x27;yoe&#x27;:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1771qcz</td>\n",
       "        <td>introducing dagster pipes</td>\n",
       "        <td>https://dagster.io/blog/dagster-pipes</td>\n",
       "        <td>schrockn</td>\n",
       "        <td>27</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>1</td>\n",
       "        <td></td>\n",
       "        <td>2023-10-13 15:45:18</td>\n",
       "        <td>Open Source</td>\n",
       "        <td></td>\n",
       "        <td>0.0</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>17754gk</td>\n",
       "        <td>python skills focus senior data engineer technical interview round</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/17754gk/what_python_skills_i_should_focus_on_for_a_senior/</td>\n",
       "        <td>mcfryme</td>\n",
       "        <td>17</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>18</td>\n",
       "        <td>I have 5+ years of Data Analysis experience. I am pretty good with SQL/PLSQL, BI tools, in python - pandas, numpy.<br><br>It&#x27;s  a one hour interview with a senior data scientist and a senior manager.  They will evaluate my SQL skills, Python and System Design.<br><br>Since  python is so vast and me having sub par skills, can you all recommend  any resources/ topics I should focus on most? I bought leetcode and  stratascratch monthly subscriptions, but the problems are overwhelming  me.<br><br>The employer is on GCP platform. Their main data engineering tools are Dataflow, Cloud Composer, Pub/Sub and Datafusion.<br><br>All responses are appreciated!</td>\n",
       "        <td>2023-10-13 18:20:38</td>\n",
       "        <td>Interview</td>\n",
       "        <td>years data analysis experience pretty good sqlplsql bi tools python pandas numpy one hour interview senior data scientist senior manager evaluate sql skills python system design since python vast sub par skills recommend resources topics focus bought leetcode stratascratch monthly subscriptions problems overwhelming employer gcp platform main data engineering tools dataflow cloud composer pubsub datafusion responses appreciated</td>\n",
       "        <td>0.8481</td>\n",
       "        <td>&#x27;5&#x27;:3 &#x27;analysi&#x27;:7 &#x27;appreci&#x27;:103 &#x27;bi&#x27;:15 &#x27;bought&#x27;:70 &#x27;cloud&#x27;:95 &#x27;compos&#x27;:96 &#x27;data&#x27;:6,30,90 &#x27;dataflow&#x27;:94 &#x27;datafus&#x27;:99 &#x27;design&#x27;:45 &#x27;employ&#x27;:83 &#x27;engin&#x27;:91 &#x27;evalu&#x27;:38 &#x27;experi&#x27;:8 &#x27;focus&#x27;:66 &#x27;gcp&#x27;:86 &#x27;good&#x27;:12 &#x27;hour&#x27;:25 &#x27;interview&#x27;:26 &#x27;leetcod&#x27;:71 &#x27;main&#x27;:89 &#x27;manag&#x27;:35 &#x27;month&#x27;:74 &#x27;numpi&#x27;:20 &#x27;one&#x27;:24 &#x27;overwhelm&#x27;:80 &#x27;panda&#x27;:19 &#x27;par&#x27;:55 &#x27;platform&#x27;:87 &#x27;pretti&#x27;:11 &#x27;problem&#x27;:78 &#x27;pub/sub&#x27;:97 &#x27;python&#x27;:18,42,47 &#x27;recommend&#x27;:60 &#x27;resourc&#x27;:62 &#x27;respons&#x27;:101 &#x27;scientist&#x27;:31 &#x27;senior&#x27;:29,34 &#x27;sinc&#x27;:46 &#x27;skill&#x27;:41,56 &#x27;sql&#x27;:40 &#x27;sql/plsql&#x27;:14 &#x27;stratascratch&#x27;:73 &#x27;sub&#x27;:54 &#x27;subscript&#x27;:75 &#x27;system&#x27;:44 &#x27;tool&#x27;:16,92 &#x27;topic&#x27;:63 &#x27;vast&#x27;:50 &#x27;year&#x27;:4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1771xvz</td>\n",
       "        <td>deploy data observability data stack</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/1771xvz/why_should_i_deploy_data_observability_for_our/</td>\n",
       "        <td>de4all</td>\n",
       "        <td>6</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>16</td>\n",
       "        <td>I can write manual scripts and run DAGs, why spends 50k+ on expensive tools?</td>\n",
       "        <td>2023-10-13 15:54:58</td>\n",
       "        <td>Discussion</td>\n",
       "        <td>write manual scripts run dags spends expensive tools</td>\n",
       "        <td>0.0</td>\n",
       "        <td>&#x27;50k&#x27;:11 &#x27;dag&#x27;:8 &#x27;expens&#x27;:13 &#x27;manual&#x27;:4 &#x27;run&#x27;:7 &#x27;script&#x27;:5 &#x27;spend&#x27;:10 &#x27;tool&#x27;:14 &#x27;write&#x27;:3</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('16x4y7c', 'monthly general discussion oct', 'https://www.reddit.com/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/', 'AutoModerator', 2, 'dataengineering', 9, 'This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find pre ... (479 characters truncated) ... ring Meetups](https://dataengineering.wiki/Community/Meetups)\\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)', datetime.datetime(2023, 10, 1, 16, 0, 58), 'Discussion', 'thread place share things might warrant thread automatically posted month find previous threads collection examples working month something accomplis ... (74 characters truncated) ... rules apply please respectful stay curious community links monthly newsletter data engineering events data engineering meetups get involved community', 0.8316, \"'/)':75 '/community/events)':81 '/community/get+involved)':95 '/community/meetups)':87 'accomplish':45 'alway':59 'appli':62 'automat':20 'collect':3 ... (503 characters truncated) ... 21 'previous':28 'recent':51 'respect':65 'rule':61 'share':9 'someth':43,48,54 'stay':67 'sub':60 'thing':10 'thread':2,17,29 'warrant':14 'work':37\"),\n",
       " ('167b3ep', 'quarterly salary discussion sep', 'https://www.reddit.com/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/', 'AutoModerator', 84, 'dataengineering', 222, \"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd\\n\\nThis is a recurring thread tha ... (341 characters truncated) ... \\n3. Location\\n4. Base salary & currency (dollars, euro, pesos, etc.)\\n5. Bonuses/Equity (optional)\\n6. Industry (optional)\\n7. Tech stack (optional)\", datetime.datetime(2023, 9, 1, 16, 1), 'Career', 'recurring thread happens quarterly created help increase transparency around salary compensation data engineering submit salary xb youd like share pu ... (48 characters truncated) ...  current title years experience yoe location base salary currency dollars euro pesos etc bonusesequity optional industry optional tech stack optional', 0.8957, \"'/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd':3 '/r/nraykn)':32 '1':51 '2':54 '3':59 '4':61 '5':69 ' ... (495 characters truncated) ... ,63 'share':38 'stack':77 'submit':26 'tally.so':31 'tally.so/r/nraykn)':30 'tech':76 'thread':8 'titl':53 'transpar':18 'well':41 'year':55 'yoe':58\"),\n",
       " ('1771qcz', 'introducing dagster pipes', 'https://dagster.io/blog/dagster-pipes', 'schrockn', 27, 'dataengineering', 1, '', datetime.datetime(2023, 10, 13, 15, 45, 18), 'Open Source', '', 0.0, ''),\n",
       " ('17754gk', 'python skills focus senior data engineer technical interview round', 'https://www.reddit.com/r/dataengineering/comments/17754gk/what_python_skills_i_should_focus_on_for_a_senior/', 'mcfryme', 17, 'dataengineering', 18, \"I have 5+ years of Data Analysis experience. I am pretty good with SQL/PLSQL, BI tools, in python - pandas, numpy.\\n\\nIt's  a one hour interview with ... (342 characters truncated) ... mployer is on GCP platform. Their main data engineering tools are Dataflow, Cloud Composer, Pub/Sub and Datafusion.\\n\\nAll responses are appreciated!\", datetime.datetime(2023, 10, 13, 18, 20, 38), 'Interview', 'years data analysis experience pretty good sqlplsql bi tools python pandas numpy one hour interview senior data scientist senior manager evaluate sql ... (133 characters truncated) ... subscriptions problems overwhelming employer gcp platform main data engineering tools dataflow cloud composer pubsub datafusion responses appreciated', 0.8481, \"'5':3 'analysi':7 'appreci':103 'bi':15 'bought':70 'cloud':95 'compos':96 'data':6,30,90 'dataflow':94 'datafus':99 'design':45 'employ':83 'engin': ... (318 characters truncated) ... ,34 'sinc':46 'skill':41,56 'sql':40 'sql/plsql':14 'stratascratch':73 'sub':54 'subscript':75 'system':44 'tool':16,92 'topic':63 'vast':50 'year':4\"),\n",
       " ('1771xvz', 'deploy data observability data stack', 'https://www.reddit.com/r/dataengineering/comments/1771xvz/why_should_i_deploy_data_observability_for_our/', 'de4all', 6, 'dataengineering', 16, 'I can write manual scripts and run DAGs, why spends 50k+ on expensive tools?', datetime.datetime(2023, 10, 13, 15, 54, 58), 'Discussion', 'write manual scripts run dags spends expensive tools', 0.0, \"'50k':11 'dag':8 'expens':13 'manual':4 'run':7 'script':5 'spend':10 'tool':14 'write':3\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "select * from mrhmr.posts_t limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>100</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(100,)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "select count(*) from mrhmr.posts_t limit 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: After you have loaded data from a subreddit, choose a few more subreddit and load those!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the project, I initially loaded data from 978 subreddits, creating a DataFrame. Since I had previously loaded 100 subreddit entries into the SQL table, I took measures to eliminate duplicate entries by cross-referencing the new DataFrame with the previous one and discarding redundant records.\n",
    "\n",
    "Next, I applied the same data preprocessing techniques to clean and refine the 'body', 'title', 'date_time', 'author', and 'subreddit' columns. I also calculated sentiment scores using a sentiment analyzer for further analysis.\n",
    "\n",
    "To accommodate the data collected in this phase, I planned to create a separate database table called 'posts_newt'. Following a similar procedure to what was done for 'posts_t', I established the table, implemented triggers, and set up a GIST index. Subsequently, I inserted the values from the DataFrame into this new SQL table.\n",
    "\n",
    "To consolidate the data from both phases and create a comprehensive dataset, I performed a union operation on the 'posts_t' and 'posts_newt' database tables, storing the result in a new database table named 'combined_posts', which now contains a total of 978 entries.\n",
    "\n",
    "Additionally, I combined the two DataFrames, 'posts_df' and 'posts_new_df', and saved the merged DataFrame as 'combined_df' in the local storage. This DataFrame, along with the 'combined_posts' database table, will be utilized in the next phase of the project for data visualization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                              title  \\\n",
      "0    16x4y7c              Monthly General Discussion - Oct 2023   \n",
      "1    167b3ep             Quarterly Salary Discussion - Sep 2023   \n",
      "2    1771qcz                          Introducing Dagster Pipes   \n",
      "3    17754gk  What python skills I should focus on for a Sen...   \n",
      "4    1771xvz  Why should I deploy data observability for our...   \n",
      "..       ...                                                ...   \n",
      "974  16aulka  Why Headless Analytics is the Game-Changer We'...   \n",
      "975  16aenrm  i was wondering is there a way to replicate a ...   \n",
      "976  16arnep  How to set up ongoing replication between Azur...   \n",
      "977  16a2c6q  Extracting from a very large amount of SQL dat...   \n",
      "978  169slfu        Been working as a data engineer for 2 years   \n",
      "\n",
      "                                                   url               author  \\\n",
      "0    https://www.reddit.com/r/dataengineering/comme...        AutoModerator   \n",
      "1    https://www.reddit.com/r/dataengineering/comme...        AutoModerator   \n",
      "2                https://dagster.io/blog/dagster-pipes             schrockn   \n",
      "3    https://www.reddit.com/r/dataengineering/comme...              mcfryme   \n",
      "4    https://www.reddit.com/r/dataengineering/comme...               de4all   \n",
      "..                                                 ...                  ...   \n",
      "974  https://lassoo.io/blog/2023/08/17/why-headless...    Euphoric-Let-8960   \n",
      "975  https://www.reddit.com/r/dataengineering/comme...  Exact-Yesterday-992   \n",
      "976  https://www.reddit.com/r/dataengineering/comme...          __hey_there   \n",
      "977  https://www.reddit.com/r/dataengineering/comme...              Peppper   \n",
      "978  https://www.reddit.com/r/dataengineering/comme...  WarNeverChanges1997   \n",
      "\n",
      "     score        subreddit  num_comments  \\\n",
      "0        2  dataengineering             9   \n",
      "1       86  dataengineering           222   \n",
      "2       27  dataengineering             1   \n",
      "3       16  dataengineering            18   \n",
      "4        7  dataengineering            16   \n",
      "..     ...              ...           ...   \n",
      "974      0  dataengineering             0   \n",
      "975      9  dataengineering            12   \n",
      "976      1  dataengineering             0   \n",
      "977     28  dataengineering            35   \n",
      "978     67  dataengineering            62   \n",
      "\n",
      "                                                  body     date_time  \\\n",
      "0    This thread is a place where you can share thi...  1.696176e+09   \n",
      "1    https://preview.redd.it/ia7kdykk8dlb1.png?widt...  1.693584e+09   \n",
      "2                                                       1.697212e+09   \n",
      "3    I have 5+ years of Data Analysis experience. I...  1.697221e+09   \n",
      "4    I can write manual scripts and run DAGs, why s...  1.697212e+09   \n",
      "..                                                 ...           ...   \n",
      "974                                                     1.693935e+09   \n",
      "975  my intention\\n\\nuse postgres to do inserts on ...  1.693890e+09   \n",
      "976  Omitting the networking, is it sufficient to s...  1.693929e+09   \n",
      "977  Assume a very large (10k+) number of on premis...  1.693858e+09   \n",
      "978  (Sorry for the long post….)\\n\\nI don’t have a ...  1.693836e+09   \n",
      "\n",
      "    link_flair_text  \n",
      "0        Discussion  \n",
      "1            Career  \n",
      "2       Open Source  \n",
      "3         Interview  \n",
      "4        Discussion  \n",
      "..              ...  \n",
      "974            Blog  \n",
      "975            Help  \n",
      "976            Help  \n",
      "977            Help  \n",
      "978      Discussion  \n",
      "\n",
      "[979 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "## Your code in this cell\n",
    "## ------------------------\n",
    "reddit = praw.Reddit(client_id='***',\n",
    "                     client_secret='***',\n",
    "                     user_agent='***')\n",
    "posts = []\n",
    "dm_subreddit = reddit.subreddit('dataengineering')\n",
    "for post in dm_subreddit.hot(limit=None):\n",
    "    posts.append([ post.id, post.title, post.url, post.author, post.score, post.subreddit, post.num_comments, post.selftext, post.created, post.link_flair_text])\n",
    "posts_new = pd.DataFrame(posts,columns=['id', 'title', 'url', 'author', 'score', 'subreddit', 'num_comments', 'body', 'date_time', 'link_flair_text'])\n",
    "print(posts_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>174hlpp</td>\n",
       "      <td>Organizational documentation for data infrastr...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>arachnarus96</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>8</td>\n",
       "      <td>Hello. I work in a two man team for a governme...</td>\n",
       "      <td>1.696933e+09</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>174ti37</td>\n",
       "      <td>Meet the MinIO Engineers: Harshavardhana - Obj...</td>\n",
       "      <td>https://www.youtube.com/watch?v=zFvR83BdAKw&amp;ut...</td>\n",
       "      <td>swodtke</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1.696965e+09</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>174jidr</td>\n",
       "      <td>Log Analysis: How to Digest 15 Billion Logs Pe...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>ApacheDoris</td>\n",
       "      <td>3</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td>If you are interested in massive data processi...</td>\n",
       "      <td>1.696940e+09</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>174q5m8</td>\n",
       "      <td>Stream Processing: Is SQL Good Enough?</td>\n",
       "      <td>https://www.risingwave.com/blog/stream-process...</td>\n",
       "      <td>yingjunwu</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1.696957e+09</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>17481kb</td>\n",
       "      <td>Airflow + DBT - question</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>yeager_doug</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\nHi there \\n\\nI’m trying to understand the...</td>\n",
       "      <td>1.696899e+09</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>16aulka</td>\n",
       "      <td>Why Headless Analytics is the Game-Changer We'...</td>\n",
       "      <td>https://lassoo.io/blog/2023/08/17/why-headless...</td>\n",
       "      <td>Euphoric-Let-8960</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1.693935e+09</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>16aenrm</td>\n",
       "      <td>i was wondering is there a way to replicate a ...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Exact-Yesterday-992</td>\n",
       "      <td>9</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>12</td>\n",
       "      <td>my intention\\n\\nuse postgres to do inserts on ...</td>\n",
       "      <td>1.693890e+09</td>\n",
       "      <td>Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>16arnep</td>\n",
       "      <td>How to set up ongoing replication between Azur...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>__hey_there</td>\n",
       "      <td>1</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td>Omitting the networking, is it sufficient to s...</td>\n",
       "      <td>1.693929e+09</td>\n",
       "      <td>Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>16a2c6q</td>\n",
       "      <td>Extracting from a very large amount of SQL dat...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Peppper</td>\n",
       "      <td>28</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>35</td>\n",
       "      <td>Assume a very large (10k+) number of on premis...</td>\n",
       "      <td>1.693858e+09</td>\n",
       "      <td>Help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>169slfu</td>\n",
       "      <td>Been working as a data engineer for 2 years</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>WarNeverChanges1997</td>\n",
       "      <td>67</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>62</td>\n",
       "      <td>(Sorry for the long post….)\\n\\nI don’t have a ...</td>\n",
       "      <td>1.693836e+09</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>879 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "100  174hlpp  Organizational documentation for data infrastr...   \n",
       "101  174ti37  Meet the MinIO Engineers: Harshavardhana - Obj...   \n",
       "102  174jidr  Log Analysis: How to Digest 15 Billion Logs Pe...   \n",
       "103  174q5m8             Stream Processing: Is SQL Good Enough?   \n",
       "104  17481kb                           Airflow + DBT - question   \n",
       "..       ...                                                ...   \n",
       "974  16aulka  Why Headless Analytics is the Game-Changer We'...   \n",
       "975  16aenrm  i was wondering is there a way to replicate a ...   \n",
       "976  16arnep  How to set up ongoing replication between Azur...   \n",
       "977  16a2c6q  Extracting from a very large amount of SQL dat...   \n",
       "978  169slfu        Been working as a data engineer for 2 years   \n",
       "\n",
       "                                                   url               author  \\\n",
       "100  https://www.reddit.com/r/dataengineering/comme...         arachnarus96   \n",
       "101  https://www.youtube.com/watch?v=zFvR83BdAKw&ut...              swodtke   \n",
       "102  https://www.reddit.com/r/dataengineering/comme...          ApacheDoris   \n",
       "103  https://www.risingwave.com/blog/stream-process...            yingjunwu   \n",
       "104  https://www.reddit.com/r/dataengineering/comme...          yeager_doug   \n",
       "..                                                 ...                  ...   \n",
       "974  https://lassoo.io/blog/2023/08/17/why-headless...    Euphoric-Let-8960   \n",
       "975  https://www.reddit.com/r/dataengineering/comme...  Exact-Yesterday-992   \n",
       "976  https://www.reddit.com/r/dataengineering/comme...          __hey_there   \n",
       "977  https://www.reddit.com/r/dataengineering/comme...              Peppper   \n",
       "978  https://www.reddit.com/r/dataengineering/comme...  WarNeverChanges1997   \n",
       "\n",
       "     score        subreddit  num_comments  \\\n",
       "100      6  dataengineering             8   \n",
       "101      0  dataengineering             0   \n",
       "102      3  dataengineering             1   \n",
       "103      0  dataengineering             0   \n",
       "104     17  dataengineering            16   \n",
       "..     ...              ...           ...   \n",
       "974      0  dataengineering             0   \n",
       "975      9  dataengineering            12   \n",
       "976      1  dataengineering             0   \n",
       "977     28  dataengineering            35   \n",
       "978     67  dataengineering            62   \n",
       "\n",
       "                                                  body     date_time  \\\n",
       "100  Hello. I work in a two man team for a governme...  1.696933e+09   \n",
       "101                                                     1.696965e+09   \n",
       "102  If you are interested in massive data processi...  1.696940e+09   \n",
       "103                                                     1.696957e+09   \n",
       "104   \\n\\nHi there \\n\\nI’m trying to understand the...  1.696899e+09   \n",
       "..                                                 ...           ...   \n",
       "974                                                     1.693935e+09   \n",
       "975  my intention\\n\\nuse postgres to do inserts on ...  1.693890e+09   \n",
       "976  Omitting the networking, is it sufficient to s...  1.693929e+09   \n",
       "977  Assume a very large (10k+) number of on premis...  1.693858e+09   \n",
       "978  (Sorry for the long post….)\\n\\nI don’t have a ...  1.693836e+09   \n",
       "\n",
       "    link_flair_text  \n",
       "100      Discussion  \n",
       "101            Blog  \n",
       "102            Blog  \n",
       "103            Blog  \n",
       "104      Discussion  \n",
       "..              ...  \n",
       "974            Blog  \n",
       "975            Help  \n",
       "976            Help  \n",
       "977            Help  \n",
       "978      Discussion  \n",
       "\n",
       "[879 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checjing for duplicate 'id's and removed them\n",
    "existing_ids = posts_df['id'].unique()\n",
    "posts_new_df = posts_new[~posts_new['id'].isin(existing_ids)]\n",
    "posts_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>174hlpp</td>\n",
       "      <td>Organizational documentation for data infrastr...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>arachnarus96</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>8</td>\n",
       "      <td>Hello. I work in a two man team for a governme...</td>\n",
       "      <td>2023-10-10 10:11:30</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>174ti37</td>\n",
       "      <td>Meet the MinIO Engineers: Harshavardhana - Obj...</td>\n",
       "      <td>https://www.youtube.com/watch?v=zFvR83BdAKw&amp;ut...</td>\n",
       "      <td>swodtke</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 19:17:02</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>174jidr</td>\n",
       "      <td>Log Analysis: How to Digest 15 Billion Logs Pe...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>ApacheDoris</td>\n",
       "      <td>3</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td>If you are interested in massive data processi...</td>\n",
       "      <td>2023-10-10 12:05:50</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>174q5m8</td>\n",
       "      <td>Stream Processing: Is SQL Good Enough?</td>\n",
       "      <td>https://www.risingwave.com/blog/stream-process...</td>\n",
       "      <td>yingjunwu</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 16:58:55</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>17481kb</td>\n",
       "      <td>Airflow + DBT - question</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>yeager_doug</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\nHi there \\n\\nI’m trying to understand the...</td>\n",
       "      <td>2023-10-10 00:43:03</td>\n",
       "      <td>Discussion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "100  174hlpp  Organizational documentation for data infrastr...   \n",
       "101  174ti37  Meet the MinIO Engineers: Harshavardhana - Obj...   \n",
       "102  174jidr  Log Analysis: How to Digest 15 Billion Logs Pe...   \n",
       "103  174q5m8             Stream Processing: Is SQL Good Enough?   \n",
       "104  17481kb                           Airflow + DBT - question   \n",
       "\n",
       "                                                   url        author  score  \\\n",
       "100  https://www.reddit.com/r/dataengineering/comme...  arachnarus96      6   \n",
       "101  https://www.youtube.com/watch?v=zFvR83BdAKw&ut...       swodtke      0   \n",
       "102  https://www.reddit.com/r/dataengineering/comme...   ApacheDoris      3   \n",
       "103  https://www.risingwave.com/blog/stream-process...     yingjunwu      0   \n",
       "104  https://www.reddit.com/r/dataengineering/comme...   yeager_doug     17   \n",
       "\n",
       "           subreddit  num_comments  \\\n",
       "100  dataengineering             8   \n",
       "101  dataengineering             0   \n",
       "102  dataengineering             1   \n",
       "103  dataengineering             0   \n",
       "104  dataengineering            16   \n",
       "\n",
       "                                                  body            date_time  \\\n",
       "100  Hello. I work in a two man team for a governme...  2023-10-10 10:11:30   \n",
       "101                                                     2023-10-10 19:17:02   \n",
       "102  If you are interested in massive data processi...  2023-10-10 12:05:50   \n",
       "103                                                     2023-10-10 16:58:55   \n",
       "104   \\n\\nHi there \\n\\nI’m trying to understand the...  2023-10-10 00:43:03   \n",
       "\n",
       "    link_flair_text  \n",
       "100      Discussion  \n",
       "101            Blog  \n",
       "102            Blog  \n",
       "103            Blog  \n",
       "104      Discussion  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting 'date_time' column to time stamp\n",
    "def unix_timestamp_to_datetime(unix_timestamp):\n",
    "    return datetime.utcfromtimestamp(unix_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "posts_new_df['date_time'] = posts_new_df['date_time'].apply(unix_timestamp_to_datetime)\n",
    "posts_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>174hlpp</td>\n",
       "      <td>Organizational documentation for data infrastr...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>arachnarus96</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>8</td>\n",
       "      <td>Hello. I work in a two man team for a governme...</td>\n",
       "      <td>2023-10-10 10:11:30</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>hello work two man team government organizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>174ti37</td>\n",
       "      <td>Meet the MinIO Engineers: Harshavardhana - Obj...</td>\n",
       "      <td>https://www.youtube.com/watch?v=zFvR83BdAKw&amp;ut...</td>\n",
       "      <td>swodtke</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 19:17:02</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>174jidr</td>\n",
       "      <td>Log Analysis: How to Digest 15 Billion Logs Pe...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>ApacheDoris</td>\n",
       "      <td>3</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td>If you are interested in massive data processi...</td>\n",
       "      <td>2023-10-10 12:05:50</td>\n",
       "      <td>Blog</td>\n",
       "      <td>interested massive data processing case might ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>174q5m8</td>\n",
       "      <td>Stream Processing: Is SQL Good Enough?</td>\n",
       "      <td>https://www.risingwave.com/blog/stream-process...</td>\n",
       "      <td>yingjunwu</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 16:58:55</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>17481kb</td>\n",
       "      <td>Airflow + DBT - question</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>yeager_doug</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\nHi there \\n\\nI’m trying to understand the...</td>\n",
       "      <td>2023-10-10 00:43:03</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>hi im trying understand real reason using airf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "100  174hlpp  Organizational documentation for data infrastr...   \n",
       "101  174ti37  Meet the MinIO Engineers: Harshavardhana - Obj...   \n",
       "102  174jidr  Log Analysis: How to Digest 15 Billion Logs Pe...   \n",
       "103  174q5m8             Stream Processing: Is SQL Good Enough?   \n",
       "104  17481kb                           Airflow + DBT - question   \n",
       "\n",
       "                                                   url        author  score  \\\n",
       "100  https://www.reddit.com/r/dataengineering/comme...  arachnarus96      6   \n",
       "101  https://www.youtube.com/watch?v=zFvR83BdAKw&ut...       swodtke      0   \n",
       "102  https://www.reddit.com/r/dataengineering/comme...   ApacheDoris      3   \n",
       "103  https://www.risingwave.com/blog/stream-process...     yingjunwu      0   \n",
       "104  https://www.reddit.com/r/dataengineering/comme...   yeager_doug     17   \n",
       "\n",
       "           subreddit  num_comments  \\\n",
       "100  dataengineering             8   \n",
       "101  dataengineering             0   \n",
       "102  dataengineering             1   \n",
       "103  dataengineering             0   \n",
       "104  dataengineering            16   \n",
       "\n",
       "                                                  body            date_time  \\\n",
       "100  Hello. I work in a two man team for a governme...  2023-10-10 10:11:30   \n",
       "101                                                     2023-10-10 19:17:02   \n",
       "102  If you are interested in massive data processi...  2023-10-10 12:05:50   \n",
       "103                                                     2023-10-10 16:58:55   \n",
       "104   \\n\\nHi there \\n\\nI’m trying to understand the...  2023-10-10 00:43:03   \n",
       "\n",
       "    link_flair_text                                  preprocessed_body  \n",
       "100      Discussion  hello work two man team government organizatio...  \n",
       "101            Blog                                                     \n",
       "102            Blog  interested massive data processing case might ...  \n",
       "103            Blog                                                     \n",
       "104      Discussion  hi im trying understand real reason using airf...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the body text\n",
    "posts_new_df['preprocessed_body'] = posts_new_df['body'] \n",
    "contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"I'm\": \"i am\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i've\": \"i have\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "     # Remove numbers along with contractions\n",
    "    text = re.sub(r'\\d+|' + '|'.join(contractions.keys()), '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    text = \" \".join([word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    # Remove single-character words\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "    \n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n",
    "posts_new_df['preprocessed_body'] = posts_new_df['preprocessed_body'].apply(preprocess_text)\n",
    "posts_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>174hlpp</td>\n",
       "      <td>organizational documentation data infrastructure</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>arachnarus96</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>8</td>\n",
       "      <td>Hello. I work in a two man team for a governme...</td>\n",
       "      <td>2023-10-10 10:11:30</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>hello work two man team government organizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>174ti37</td>\n",
       "      <td>meet minio engineers harshavardhana object han...</td>\n",
       "      <td>https://www.youtube.com/watch?v=zFvR83BdAKw&amp;ut...</td>\n",
       "      <td>swodtke</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 19:17:02</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>174jidr</td>\n",
       "      <td>log analysis digest billion logs per day keep ...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>ApacheDoris</td>\n",
       "      <td>3</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td>If you are interested in massive data processi...</td>\n",
       "      <td>2023-10-10 12:05:50</td>\n",
       "      <td>Blog</td>\n",
       "      <td>interested massive data processing case might ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>174q5m8</td>\n",
       "      <td>stream processing sql good enough</td>\n",
       "      <td>https://www.risingwave.com/blog/stream-process...</td>\n",
       "      <td>yingjunwu</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 16:58:55</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>17481kb</td>\n",
       "      <td>airflow dbt question</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>yeager_doug</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\nHi there \\n\\nI’m trying to understand the...</td>\n",
       "      <td>2023-10-10 00:43:03</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>hi im trying understand real reason using airf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>16aulka</td>\n",
       "      <td>headless analytics gamechanger weve waiting</td>\n",
       "      <td>https://lassoo.io/blog/2023/08/17/why-headless...</td>\n",
       "      <td>Euphoric-Let-8960</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-09-05 17:35:27</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>16aenrm</td>\n",
       "      <td>wondering way replicate postgres database anot...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Exact-Yesterday-992</td>\n",
       "      <td>9</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>12</td>\n",
       "      <td>my intention\\n\\nuse postgres to do inserts on ...</td>\n",
       "      <td>2023-09-05 04:53:25</td>\n",
       "      <td>Help</td>\n",
       "      <td>intention use postgres inserts data use differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>16arnep</td>\n",
       "      <td>set ongoing replication azure sql server aws r...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>__hey_there</td>\n",
       "      <td>1</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td>Omitting the networking, is it sufficient to s...</td>\n",
       "      <td>2023-09-05 15:42:36</td>\n",
       "      <td>Help</td>\n",
       "      <td>omitting networking sufficient set data sync a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>16a2c6q</td>\n",
       "      <td>extracting large amount sql databases</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Peppper</td>\n",
       "      <td>28</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>35</td>\n",
       "      <td>Assume a very large (10k+) number of on premis...</td>\n",
       "      <td>2023-09-04 20:02:38</td>\n",
       "      <td>Help</td>\n",
       "      <td>assume large number premise single tenant data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>169slfu</td>\n",
       "      <td>working data engineer years</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>WarNeverChanges1997</td>\n",
       "      <td>67</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>62</td>\n",
       "      <td>(Sorry for the long post….)\\n\\nI don’t have a ...</td>\n",
       "      <td>2023-09-04 13:53:01</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>sorry long post dont technical background star...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>879 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "100  174hlpp   organizational documentation data infrastructure   \n",
       "101  174ti37  meet minio engineers harshavardhana object han...   \n",
       "102  174jidr  log analysis digest billion logs per day keep ...   \n",
       "103  174q5m8                  stream processing sql good enough   \n",
       "104  17481kb                               airflow dbt question   \n",
       "..       ...                                                ...   \n",
       "974  16aulka        headless analytics gamechanger weve waiting   \n",
       "975  16aenrm  wondering way replicate postgres database anot...   \n",
       "976  16arnep  set ongoing replication azure sql server aws r...   \n",
       "977  16a2c6q              extracting large amount sql databases   \n",
       "978  169slfu                        working data engineer years   \n",
       "\n",
       "                                                   url               author  \\\n",
       "100  https://www.reddit.com/r/dataengineering/comme...         arachnarus96   \n",
       "101  https://www.youtube.com/watch?v=zFvR83BdAKw&ut...              swodtke   \n",
       "102  https://www.reddit.com/r/dataengineering/comme...          ApacheDoris   \n",
       "103  https://www.risingwave.com/blog/stream-process...            yingjunwu   \n",
       "104  https://www.reddit.com/r/dataengineering/comme...          yeager_doug   \n",
       "..                                                 ...                  ...   \n",
       "974  https://lassoo.io/blog/2023/08/17/why-headless...    Euphoric-Let-8960   \n",
       "975  https://www.reddit.com/r/dataengineering/comme...  Exact-Yesterday-992   \n",
       "976  https://www.reddit.com/r/dataengineering/comme...          __hey_there   \n",
       "977  https://www.reddit.com/r/dataengineering/comme...              Peppper   \n",
       "978  https://www.reddit.com/r/dataengineering/comme...  WarNeverChanges1997   \n",
       "\n",
       "     score        subreddit  num_comments  \\\n",
       "100      6  dataengineering             8   \n",
       "101      0  dataengineering             0   \n",
       "102      3  dataengineering             1   \n",
       "103      0  dataengineering             0   \n",
       "104     17  dataengineering            16   \n",
       "..     ...              ...           ...   \n",
       "974      0  dataengineering             0   \n",
       "975      9  dataengineering            12   \n",
       "976      1  dataengineering             0   \n",
       "977     28  dataengineering            35   \n",
       "978     67  dataengineering            62   \n",
       "\n",
       "                                                  body            date_time  \\\n",
       "100  Hello. I work in a two man team for a governme...  2023-10-10 10:11:30   \n",
       "101                                                     2023-10-10 19:17:02   \n",
       "102  If you are interested in massive data processi...  2023-10-10 12:05:50   \n",
       "103                                                     2023-10-10 16:58:55   \n",
       "104   \\n\\nHi there \\n\\nI’m trying to understand the...  2023-10-10 00:43:03   \n",
       "..                                                 ...                  ...   \n",
       "974                                                     2023-09-05 17:35:27   \n",
       "975  my intention\\n\\nuse postgres to do inserts on ...  2023-09-05 04:53:25   \n",
       "976  Omitting the networking, is it sufficient to s...  2023-09-05 15:42:36   \n",
       "977  Assume a very large (10k+) number of on premis...  2023-09-04 20:02:38   \n",
       "978  (Sorry for the long post….)\\n\\nI don’t have a ...  2023-09-04 13:53:01   \n",
       "\n",
       "    link_flair_text                                  preprocessed_body  \n",
       "100      Discussion  hello work two man team government organizatio...  \n",
       "101            Blog                                                     \n",
       "102            Blog  interested massive data processing case might ...  \n",
       "103            Blog                                                     \n",
       "104      Discussion  hi im trying understand real reason using airf...  \n",
       "..              ...                                                ...  \n",
       "974            Blog                                                     \n",
       "975            Help  intention use postgres inserts data use differ...  \n",
       "976            Help  omitting networking sufficient set data sync a...  \n",
       "977            Help  assume large number premise single tenant data...  \n",
       "978      Discussion  sorry long post dont technical background star...  \n",
       "\n",
       "[879 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing 'title' column\n",
    "def preprocess_text(text):\n",
    "    # removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # removing punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "     # removing numbers along with contractions\n",
    "    text = re.sub(r'\\d+|' + '|'.join(contractions.keys()), '', text)\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    text = \" \".join([word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    # removing single-character words\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "    \n",
    "    # removing newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "posts_new_df['title'] = posts_new_df['title'].apply(preprocess_text)\n",
    "posts_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>174hlpp</td>\n",
       "      <td>organizational documentation data infrastructure</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>arachnarus96</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>8</td>\n",
       "      <td>Hello. I work in a two man team for a governme...</td>\n",
       "      <td>2023-10-10 10:11:30</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>hello work two man team government organizatio...</td>\n",
       "      <td>0.9567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>174ti37</td>\n",
       "      <td>meet minio engineers harshavardhana object han...</td>\n",
       "      <td>https://www.youtube.com/watch?v=zFvR83BdAKw&amp;ut...</td>\n",
       "      <td>swodtke</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 19:17:02</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>174jidr</td>\n",
       "      <td>log analysis digest billion logs per day keep ...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>ApacheDoris</td>\n",
       "      <td>3</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td>If you are interested in massive data processi...</td>\n",
       "      <td>2023-10-10 12:05:50</td>\n",
       "      <td>Blog</td>\n",
       "      <td>interested massive data processing case might ...</td>\n",
       "      <td>0.6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>174q5m8</td>\n",
       "      <td>stream processing sql good enough</td>\n",
       "      <td>https://www.risingwave.com/blog/stream-process...</td>\n",
       "      <td>yingjunwu</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-10 16:58:55</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>17481kb</td>\n",
       "      <td>airflow dbt question</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>yeager_doug</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\nHi there \\n\\nI’m trying to understand the...</td>\n",
       "      <td>2023-10-10 00:43:03</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>hi im trying understand real reason using airf...</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "100  174hlpp   organizational documentation data infrastructure   \n",
       "101  174ti37  meet minio engineers harshavardhana object han...   \n",
       "102  174jidr  log analysis digest billion logs per day keep ...   \n",
       "103  174q5m8                  stream processing sql good enough   \n",
       "104  17481kb                               airflow dbt question   \n",
       "\n",
       "                                                   url        author  score  \\\n",
       "100  https://www.reddit.com/r/dataengineering/comme...  arachnarus96      6   \n",
       "101  https://www.youtube.com/watch?v=zFvR83BdAKw&ut...       swodtke      0   \n",
       "102  https://www.reddit.com/r/dataengineering/comme...   ApacheDoris      3   \n",
       "103  https://www.risingwave.com/blog/stream-process...     yingjunwu      0   \n",
       "104  https://www.reddit.com/r/dataengineering/comme...   yeager_doug     17   \n",
       "\n",
       "           subreddit  num_comments  \\\n",
       "100  dataengineering             8   \n",
       "101  dataengineering             0   \n",
       "102  dataengineering             1   \n",
       "103  dataengineering             0   \n",
       "104  dataengineering            16   \n",
       "\n",
       "                                                  body            date_time  \\\n",
       "100  Hello. I work in a two man team for a governme...  2023-10-10 10:11:30   \n",
       "101                                                     2023-10-10 19:17:02   \n",
       "102  If you are interested in massive data processi...  2023-10-10 12:05:50   \n",
       "103                                                     2023-10-10 16:58:55   \n",
       "104   \\n\\nHi there \\n\\nI’m trying to understand the...  2023-10-10 00:43:03   \n",
       "\n",
       "    link_flair_text                                  preprocessed_body  \\\n",
       "100      Discussion  hello work two man team government organizatio...   \n",
       "101            Blog                                                      \n",
       "102            Blog  interested massive data processing case might ...   \n",
       "103            Blog                                                      \n",
       "104      Discussion  hi im trying understand real reason using airf...   \n",
       "\n",
       "     sentiment_score  \n",
       "100           0.9567  \n",
       "101           0.0000  \n",
       "102           0.6597  \n",
       "103           0.0000  \n",
       "104           0.4215  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment analysis on 'preprocessed_body'  column, calculated compound score and saved it as 'sentiment_score'\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    sentiment_scores = sa.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "# Apply sentiment analysis to the 'ir_content' column\n",
    "posts_new_df['sentiment_score'] = posts_new_df['preprocessed_body'].apply(calculate_sentiment)\n",
    "posts_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#converted the 'author' and 'subreddit' column to standardize the data to readable format\n",
    "posts_new_df['author'] = posts_new_df['author'].apply(lambda redditor: redditor.name if hasattr(redditor, 'name') else None)\n",
    "posts_new_df['subreddit'] = posts_new_df['subreddit'].apply(lambda subreddit: subreddit.display_name if hasattr(subreddit, 'display_name') else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Database table creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS posts_newt;\n",
    "\n",
    "CREATE TABLE posts_newt(\n",
    "    id TEXT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    url TEXT,\n",
    "    author TEXT,\n",
    "    score INTEGER,\n",
    "    subreddit TEXT,\n",
    "    num_comments INTEGER,\n",
    "    body TEXT,\n",
    "    date_time TIMESTAMP,\n",
    "    link_flair_text TEXT,\n",
    "    preprocessed_body TEXT,\n",
    "    sentiment_score double precision\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TRIGGER IF EXISTS tsv_gist_update on posts_newt;\n",
    "    \n",
    "CREATE TRIGGER tsv_gist_update \n",
    "    BEFORE INSERT OR UPDATE\n",
    "    ON posts_newt \n",
    "    FOR EACH ROW \n",
    "    EXECUTE PROCEDURE\n",
    "    tsvector_update_trigger(body_tsv_gist,'pg_catalog.english', body);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "-- TS_Vector for GIST INDEX\n",
    "ALTER TABLE posts_newt\n",
    "  ADD COLUMN body_tsv_gist tsvector;\n",
    "\n",
    "-- now update the above column by parsing the content column. Note: the following is only required if we\n",
    "-- already have some rows in the table.\n",
    "\n",
    "UPDATE posts_newt \n",
    "SET body_tsv_gist = to_tsvector('pg_catalog.english', preprocessed_body);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_new_df.to_sql('posts_newt', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>879</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(879,)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) FROM mrhmr.posts_newt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>title</th>\n",
       "        <th>url</th>\n",
       "        <th>author</th>\n",
       "        <th>score</th>\n",
       "        <th>subreddit</th>\n",
       "        <th>num_comments</th>\n",
       "        <th>body</th>\n",
       "        <th>date_time</th>\n",
       "        <th>link_flair_text</th>\n",
       "        <th>preprocessed_body</th>\n",
       "        <th>sentiment_score</th>\n",
       "        <th>body_tsv_gist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>174hlpp</td>\n",
       "        <td>organizational documentation data infrastructure</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/174hlpp/organizational_documentation_for_data/</td>\n",
       "        <td>arachnarus96</td>\n",
       "        <td>6</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>8</td>\n",
       "        <td>Hello. I work in a two man team for a government organization as an all purpose data engineer. Meaning we set up and maintain all data pipelines, the databases, the data reports and do machine learning projects when we have time. The methodology for the last three years, (yes the organization just started to think about data three years ago), has been a low hanging fruit methodology, meaning we start a project, create some value from it, publish it and move on. <br><br>Needless to say that has left a lot of quality control neglected. Name giving is inconsistent, data owners and users are often unknown and some quick fix sh\\*t solutions are still being used like windows scheduler to run some codes. There is hardly any documentation about our data infrastructure to add. <br><br>Now I don&#x27;t think any one is to blame for this as this is a government organization on a budget and the two of us are head over heels in projects but the time has come to tighten loose ends. My question is, has anyone experienced a similar scenario and solved it? How did you solve it? Is there any good literature on the subject or other resources? <br><br>FYI we are using Microsoft solutions like Azure and power platform for 90% of what we do. We are also a REIT and construction management type of organization if that is relevant.<br><br>Thanks a lot in advance for all responses.</td>\n",
       "        <td>2023-10-10 10:11:30</td>\n",
       "        <td>Discussion</td>\n",
       "        <td>hello work two man team government organization purpose data engineer meaning set maintain data pipelines databases data reports machine learning projects time methodology last three years yes organization started think data three years ago low hanging fruit methodology meaning start project create value publish move needless say left lot quality control neglected name giving inconsistent data owners users often unknown quick fix sht solutions still used like windows scheduler run codes hardly documentation data infrastructure add dont think one blame government organization budget two us head heels projects time come tighten loose ends question anyone experienced similar scenario solved solve good literature subject resources fyi using microsoft solutions like azure power platform also reit construction management type organization relevant thanks lot advance responses</td>\n",
       "        <td>0.9567</td>\n",
       "        <td>&#x27;90&#x27;:216 &#x27;add&#x27;:134 &#x27;advanc&#x27;:240 &#x27;ago&#x27;:60 &#x27;also&#x27;:223 &#x27;anyon&#x27;:180 &#x27;azur&#x27;:211 &#x27;blame&#x27;:144 &#x27;budget&#x27;:155 &#x27;code&#x27;:123 &#x27;come&#x27;:171 &#x27;construct&#x27;:227 &#x27;control&#x27;:93 &#x27;creat&#x27;:73 &#x27;data&#x27;:17,26,31,57,99,131 &#x27;databas&#x27;:29 &#x27;document&#x27;:128 &#x27;end&#x27;:175 &#x27;engin&#x27;:18 &#x27;experienc&#x27;:181 &#x27;fix&#x27;:109 &#x27;fruit&#x27;:66 &#x27;fyi&#x27;:204 &#x27;give&#x27;:96 &#x27;good&#x27;:196 &#x27;govern&#x27;:11,151 &#x27;hang&#x27;:65 &#x27;hard&#x27;:126 &#x27;head&#x27;:162 &#x27;heel&#x27;:164 &#x27;hello&#x27;:1 &#x27;inconsist&#x27;:98 &#x27;infrastructur&#x27;:132 &#x27;last&#x27;:46 &#x27;learn&#x27;:36 &#x27;left&#x27;:88 &#x27;like&#x27;:117,210 &#x27;literatur&#x27;:197 &#x27;loos&#x27;:174 &#x27;lot&#x27;:90,238 &#x27;low&#x27;:64 &#x27;machin&#x27;:35 &#x27;maintain&#x27;:24 &#x27;man&#x27;:7 &#x27;manag&#x27;:228 &#x27;mean&#x27;:19,68 &#x27;methodolog&#x27;:43,67 &#x27;microsoft&#x27;:208 &#x27;move&#x27;:81 &#x27;name&#x27;:95 &#x27;needless&#x27;:83 &#x27;neglect&#x27;:94 &#x27;often&#x27;:104 &#x27;one&#x27;:141 &#x27;organ&#x27;:12,51,152,231 &#x27;owner&#x27;:100 &#x27;pipelin&#x27;:27 &#x27;platform&#x27;:214 &#x27;power&#x27;:213 &#x27;project&#x27;:37,72,166 &#x27;publish&#x27;:78 &#x27;purpos&#x27;:16 &#x27;qualiti&#x27;:92 &#x27;question&#x27;:177 &#x27;quick&#x27;:108 &#x27;reit&#x27;:225 &#x27;relev&#x27;:235 &#x27;report&#x27;:32 &#x27;resourc&#x27;:203 &#x27;respons&#x27;:243 &#x27;run&#x27;:121 &#x27;say&#x27;:85 &#x27;scenario&#x27;:184 &#x27;schedul&#x27;:119 &#x27;set&#x27;:21 &#x27;sh&#x27;:110 &#x27;similar&#x27;:183 &#x27;solut&#x27;:112,209 &#x27;solv&#x27;:186,191 &#x27;start&#x27;:53,70 &#x27;still&#x27;:114 &#x27;subject&#x27;:200 &#x27;team&#x27;:8 &#x27;thank&#x27;:236 &#x27;think&#x27;:55,139 &#x27;three&#x27;:47,58 &#x27;tighten&#x27;:173 &#x27;time&#x27;:41,169 &#x27;two&#x27;:6,158 &#x27;type&#x27;:229 &#x27;unknown&#x27;:105 &#x27;us&#x27;:160 &#x27;use&#x27;:116,207 &#x27;user&#x27;:102 &#x27;valu&#x27;:75 &#x27;window&#x27;:118 &#x27;work&#x27;:3 &#x27;year&#x27;:48,59 &#x27;yes&#x27;:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>174ti37</td>\n",
       "        <td>meet minio engineers harshavardhana object handling</td>\n",
       "        <td>https://www.youtube.com/watch?v=zFvR83BdAKw&amp;utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=meet_engineers_harsha+</td>\n",
       "        <td>swodtke</td>\n",
       "        <td>0</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>0</td>\n",
       "        <td></td>\n",
       "        <td>2023-10-10 19:17:02</td>\n",
       "        <td>Blog</td>\n",
       "        <td></td>\n",
       "        <td>0.0</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>174jidr</td>\n",
       "        <td>log analysis digest billion logs per day keep big queries within second</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/174jidr/log_analysis_how_to_digest_15_billion_logs_per/</td>\n",
       "        <td>ApacheDoris</td>\n",
       "        <td>3</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>1</td>\n",
       "        <td>If you are interested in massive data processing, [this case](https://doris.apache.org/zh-CN/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second) might help.<br><br>https://preview.redd.it/cbo62wez7dtb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=bfddfc33093973663168acaa2faec12eacf0f460</td>\n",
       "        <td>2023-10-10 12:05:50</td>\n",
       "        <td>Blog</td>\n",
       "        <td>interested massive data processing case might help</td>\n",
       "        <td>0.6597</td>\n",
       "        <td>&#x27;/cbo62wez7dtb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=bfddfc33093973663168acaa2faec12eacf0f460&#x27;:18 &#x27;/zh-cn/blog/log-analysis-how-to-digest-15-billion-logs-per-day-and-keep-big-queries-within-1-second)&#x27;:13 &#x27;case&#x27;:10 &#x27;data&#x27;:7 &#x27;doris.apache.org&#x27;:12 &#x27;doris.apache.org/zh-cn/blog/log-analysis-how-to-digest-15-billion-logs-per-day-and-keep-big-queries-within-1-second)&#x27;:11 &#x27;help&#x27;:15 &#x27;interest&#x27;:4 &#x27;massiv&#x27;:6 &#x27;might&#x27;:14 &#x27;preview.redd.it&#x27;:17 &#x27;preview.redd.it/cbo62wez7dtb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=bfddfc33093973663168acaa2faec12eacf0f460&#x27;:16 &#x27;process&#x27;:8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>174q5m8</td>\n",
       "        <td>stream processing sql good enough</td>\n",
       "        <td>https://www.risingwave.com/blog/stream-processing-is-sql-good-enough/</td>\n",
       "        <td>yingjunwu</td>\n",
       "        <td>0</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>0</td>\n",
       "        <td></td>\n",
       "        <td>2023-10-10 16:58:55</td>\n",
       "        <td>Blog</td>\n",
       "        <td></td>\n",
       "        <td>0.0</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>17481kb</td>\n",
       "        <td>airflow dbt question</td>\n",
       "        <td>https://www.reddit.com/r/dataengineering/comments/17481kb/airflow_dbt_question/</td>\n",
       "        <td>yeager_doug</td>\n",
       "        <td>17</td>\n",
       "        <td>dataengineering</td>\n",
       "        <td>16</td>\n",
       "        <td> <br><br>Hi there <br><br>I’m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  <br> <br><br>Is DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?</td>\n",
       "        <td>2023-10-10 00:43:03</td>\n",
       "        <td>Discussion</td>\n",
       "        <td>hi im trying understand real reason using airflow dbt first one connect directly database apply necessary transformations using operators postgres redshift etc dbt adding complexity project helpful ways cannot find</td>\n",
       "        <td>0.4215</td>\n",
       "        <td>&#x27;ad&#x27;:39 &#x27;airflow&#x27;:13 &#x27;appli&#x27;:26 &#x27;cannot&#x27;:55 &#x27;complex&#x27;:41 &#x27;connect&#x27;:20 &#x27;databas&#x27;:24 &#x27;dbt&#x27;:14,37 &#x27;direct&#x27;:21 &#x27;etc&#x27;:35 &#x27;find&#x27;:56 &#x27;first&#x27;:17 &#x27;help&#x27;:49 &#x27;hi&#x27;:1 &#x27;m&#x27;:4 &#x27;necessari&#x27;:28 &#x27;one&#x27;:18 &#x27;oper&#x27;:32 &#x27;postgr&#x27;:33 &#x27;project&#x27;:44 &#x27;real&#x27;:9 &#x27;reason&#x27;:10 &#x27;redshift&#x27;:34 &#x27;transform&#x27;:29 &#x27;tri&#x27;:5 &#x27;understand&#x27;:7 &#x27;use&#x27;:12,31 &#x27;way&#x27;:52</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('174hlpp', 'organizational documentation data infrastructure', 'https://www.reddit.com/r/dataengineering/comments/174hlpp/organizational_documentation_for_data/', 'arachnarus96', 6, 'dataengineering', 8, \"Hello. I work in a two man team for a government organization as an all purpose data engineer. Meaning we set up and maintain all data pipelines, the ... (1060 characters truncated) ... of what we do. We are also a REIT and construction management type of organization if that is relevant.\\n\\nThanks a lot in advance for all responses.\", datetime.datetime(2023, 10, 10, 10, 11, 30), 'Discussion', 'hello work two man team government organization purpose data engineer meaning set maintain data pipelines databases data reports machine learning pro ... (584 characters truncated) ... ces fyi using microsoft solutions like azure power platform also reit construction management type organization relevant thanks lot advance responses', 0.9567, \"'90':216 'add':134 'advanc':240 'ago':60 'also':223 'anyon':180 'azur':211 'blame':144 'budget':155 'code':123 'come':171 'construct':227 'control':9 ... (944 characters truncated) ... ghten':173 'time':41,169 'two':6,158 'type':229 'unknown':105 'us':160 'use':116,207 'user':102 'valu':75 'window':118 'work':3 'year':48,59 'yes':49\"),\n",
       " ('174ti37', 'meet minio engineers harshavardhana object handling', 'https://www.youtube.com/watch?v=zFvR83BdAKw&utm_source=reddit&utm_medium=organic-social+&utm_campaign=meet_engineers_harsha+', 'swodtke', 0, 'dataengineering', 0, '', datetime.datetime(2023, 10, 10, 19, 17, 2), 'Blog', '', 0.0, ''),\n",
       " ('174jidr', 'log analysis digest billion logs per day keep big queries within second', 'https://www.reddit.com/r/dataengineering/comments/174jidr/log_analysis_how_to_digest_15_billion_logs_per/', 'ApacheDoris', 3, 'dataengineering', 1, 'If you are interested in massive data processing, [this case](https://doris.apache.org/zh-CN/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day- ... (20 characters truncated) ... -Within-1-Second) might help.\\n\\nhttps://preview.redd.it/cbo62wez7dtb1.png?width=1280&format=png&auto=webp&s=bfddfc33093973663168acaa2faec12eacf0f460', datetime.datetime(2023, 10, 10, 12, 5, 50), 'Blog', 'interested massive data processing case might help', 0.6597, \"'/cbo62wez7dtb1.png?width=1280&format=png&auto=webp&s=bfddfc33093973663168acaa2faec12eacf0f460':18 '/zh-cn/blog/log-analysis-how-to-digest-15-billion ... (261 characters truncated) ... 14 'preview.redd.it':17 'preview.redd.it/cbo62wez7dtb1.png?width=1280&format=png&auto=webp&s=bfddfc33093973663168acaa2faec12eacf0f460':16 'process':8\"),\n",
       " ('174q5m8', 'stream processing sql good enough', 'https://www.risingwave.com/blog/stream-processing-is-sql-good-enough/', 'yingjunwu', 0, 'dataengineering', 0, '', datetime.datetime(2023, 10, 10, 16, 58, 55), 'Blog', '', 0.0, ''),\n",
       " ('17481kb', 'airflow dbt question', 'https://www.reddit.com/r/dataengineering/comments/17481kb/airflow_dbt_question/', 'yeager_doug', 17, 'dataengineering', 16, ' \\n\\nHi there \\n\\nI’m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply a ... (43 characters truncated) ... tors (Postgres, Redshift, etc).  \\n \\n\\nIs DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?', datetime.datetime(2023, 10, 10, 0, 43, 3), 'Discussion', 'hi im trying understand real reason using airflow dbt first one connect directly database apply necessary transformations using operators postgres redshift etc dbt adding complexity project helpful ways cannot find', 0.4215, \"'ad':39 'airflow':13 'appli':26 'cannot':55 'complex':41 'connect':20 'databas':24 'dbt':14,37 'direct':21 'etc':35 'find':56 'first':17 'help':49 'h ... (14 characters truncated) ... cessari':28 'one':18 'oper':32 'postgr':33 'project':44 'real':9 'reason':10 'redshift':34 'transform':29 'tri':5 'understand':7 'use':12,31 'way':52\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "select * from mrhmr.posts_newt limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "979 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS combined_posts;\n",
    "CREATE TABLE combined_posts AS\n",
    "SELECT * FROM posts_t\n",
    "UNION ALL\n",
    "SELECT * FROM posts_newt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([posts_df, posts_new_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>date_time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>preprocessed_body</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16x4y7c</td>\n",
       "      <td>monthly general discussion oct</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>2</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>9</td>\n",
       "      <td>This thread is a place where you can share thi...</td>\n",
       "      <td>2023-10-01 16:00:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>thread place share things might warrant thread...</td>\n",
       "      <td>0.8316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167b3ep</td>\n",
       "      <td>quarterly salary discussion sep</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>84</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>222</td>\n",
       "      <td>https://preview.redd.it/ia7kdykk8dlb1.png?widt...</td>\n",
       "      <td>2023-09-01 16:01:00</td>\n",
       "      <td>Career</td>\n",
       "      <td>recurring thread happens quarterly created hel...</td>\n",
       "      <td>0.8957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1771qcz</td>\n",
       "      <td>introducing dagster pipes</td>\n",
       "      <td>https://dagster.io/blog/dagster-pipes</td>\n",
       "      <td>schrockn</td>\n",
       "      <td>27</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-10-13 15:45:18</td>\n",
       "      <td>Open Source</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17754gk</td>\n",
       "      <td>python skills focus senior data engineer techn...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>mcfryme</td>\n",
       "      <td>17</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>18</td>\n",
       "      <td>I have 5+ years of Data Analysis experience. I...</td>\n",
       "      <td>2023-10-13 18:20:38</td>\n",
       "      <td>Interview</td>\n",
       "      <td>years data analysis experience pretty good sql...</td>\n",
       "      <td>0.8481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1771xvz</td>\n",
       "      <td>deploy data observability data stack</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>de4all</td>\n",
       "      <td>6</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>16</td>\n",
       "      <td>I can write manual scripts and run DAGs, why s...</td>\n",
       "      <td>2023-10-13 15:54:58</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>write manual scripts run dags spends expensive...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>16aulka</td>\n",
       "      <td>headless analytics gamechanger weve waiting</td>\n",
       "      <td>https://lassoo.io/blog/2023/08/17/why-headless...</td>\n",
       "      <td>Euphoric-Let-8960</td>\n",
       "      <td>0</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2023-09-05 17:35:27</td>\n",
       "      <td>Blog</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>16aenrm</td>\n",
       "      <td>wondering way replicate postgres database anot...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Exact-Yesterday-992</td>\n",
       "      <td>9</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>12</td>\n",
       "      <td>my intention\\n\\nuse postgres to do inserts on ...</td>\n",
       "      <td>2023-09-05 04:53:25</td>\n",
       "      <td>Help</td>\n",
       "      <td>intention use postgres inserts data use differ...</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>16arnep</td>\n",
       "      <td>set ongoing replication azure sql server aws r...</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>__hey_there</td>\n",
       "      <td>1</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>0</td>\n",
       "      <td>Omitting the networking, is it sufficient to s...</td>\n",
       "      <td>2023-09-05 15:42:36</td>\n",
       "      <td>Help</td>\n",
       "      <td>omitting networking sufficient set data sync a...</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>16a2c6q</td>\n",
       "      <td>extracting large amount sql databases</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>Peppper</td>\n",
       "      <td>28</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>35</td>\n",
       "      <td>Assume a very large (10k+) number of on premis...</td>\n",
       "      <td>2023-09-04 20:02:38</td>\n",
       "      <td>Help</td>\n",
       "      <td>assume large number premise single tenant data...</td>\n",
       "      <td>0.9846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>169slfu</td>\n",
       "      <td>working data engineer years</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>WarNeverChanges1997</td>\n",
       "      <td>67</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>62</td>\n",
       "      <td>(Sorry for the long post….)\\n\\nI don’t have a ...</td>\n",
       "      <td>2023-09-04 13:53:01</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>sorry long post dont technical background star...</td>\n",
       "      <td>0.9951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>979 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0    16x4y7c                     monthly general discussion oct   \n",
       "1    167b3ep                    quarterly salary discussion sep   \n",
       "2    1771qcz                          introducing dagster pipes   \n",
       "3    17754gk  python skills focus senior data engineer techn...   \n",
       "4    1771xvz               deploy data observability data stack   \n",
       "..       ...                                                ...   \n",
       "974  16aulka        headless analytics gamechanger weve waiting   \n",
       "975  16aenrm  wondering way replicate postgres database anot...   \n",
       "976  16arnep  set ongoing replication azure sql server aws r...   \n",
       "977  16a2c6q              extracting large amount sql databases   \n",
       "978  169slfu                        working data engineer years   \n",
       "\n",
       "                                                   url               author  \\\n",
       "0    https://www.reddit.com/r/dataengineering/comme...        AutoModerator   \n",
       "1    https://www.reddit.com/r/dataengineering/comme...        AutoModerator   \n",
       "2                https://dagster.io/blog/dagster-pipes             schrockn   \n",
       "3    https://www.reddit.com/r/dataengineering/comme...              mcfryme   \n",
       "4    https://www.reddit.com/r/dataengineering/comme...               de4all   \n",
       "..                                                 ...                  ...   \n",
       "974  https://lassoo.io/blog/2023/08/17/why-headless...    Euphoric-Let-8960   \n",
       "975  https://www.reddit.com/r/dataengineering/comme...  Exact-Yesterday-992   \n",
       "976  https://www.reddit.com/r/dataengineering/comme...          __hey_there   \n",
       "977  https://www.reddit.com/r/dataengineering/comme...              Peppper   \n",
       "978  https://www.reddit.com/r/dataengineering/comme...  WarNeverChanges1997   \n",
       "\n",
       "     score        subreddit  num_comments  \\\n",
       "0        2  dataengineering             9   \n",
       "1       84  dataengineering           222   \n",
       "2       27  dataengineering             1   \n",
       "3       17  dataengineering            18   \n",
       "4        6  dataengineering            16   \n",
       "..     ...              ...           ...   \n",
       "974      0  dataengineering             0   \n",
       "975      9  dataengineering            12   \n",
       "976      1  dataengineering             0   \n",
       "977     28  dataengineering            35   \n",
       "978     67  dataengineering            62   \n",
       "\n",
       "                                                  body            date_time  \\\n",
       "0    This thread is a place where you can share thi...  2023-10-01 16:00:58   \n",
       "1    https://preview.redd.it/ia7kdykk8dlb1.png?widt...  2023-09-01 16:01:00   \n",
       "2                                                       2023-10-13 15:45:18   \n",
       "3    I have 5+ years of Data Analysis experience. I...  2023-10-13 18:20:38   \n",
       "4    I can write manual scripts and run DAGs, why s...  2023-10-13 15:54:58   \n",
       "..                                                 ...                  ...   \n",
       "974                                                     2023-09-05 17:35:27   \n",
       "975  my intention\\n\\nuse postgres to do inserts on ...  2023-09-05 04:53:25   \n",
       "976  Omitting the networking, is it sufficient to s...  2023-09-05 15:42:36   \n",
       "977  Assume a very large (10k+) number of on premis...  2023-09-04 20:02:38   \n",
       "978  (Sorry for the long post….)\\n\\nI don’t have a ...  2023-09-04 13:53:01   \n",
       "\n",
       "    link_flair_text                                  preprocessed_body  \\\n",
       "0        Discussion  thread place share things might warrant thread...   \n",
       "1            Career  recurring thread happens quarterly created hel...   \n",
       "2       Open Source                                                      \n",
       "3         Interview  years data analysis experience pretty good sql...   \n",
       "4        Discussion  write manual scripts run dags spends expensive...   \n",
       "..              ...                                                ...   \n",
       "974            Blog                                                      \n",
       "975            Help  intention use postgres inserts data use differ...   \n",
       "976            Help  omitting networking sufficient set data sync a...   \n",
       "977            Help  assume large number premise single tenant data...   \n",
       "978      Discussion  sorry long post dont technical background star...   \n",
       "\n",
       "     sentiment_score  \n",
       "0             0.8316  \n",
       "1             0.8957  \n",
       "2             0.0000  \n",
       "3             0.8481  \n",
       "4             0.0000  \n",
       "..               ...  \n",
       "974           0.0000  \n",
       "975          -0.4767  \n",
       "976          -0.2960  \n",
       "977           0.9846  \n",
       "978           0.9951  \n",
       "\n",
       "[979 rows x 12 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://mrhmr:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "GRANT USAGE ON SCHEMA *** TO ***;\n",
    "GRANT SELECT ON combined_posts TO ***;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
